{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!wget https://github.com/googly-mingto/ML2023HW4/releases/download/data/Dataset.tar.gz.partaa\n!wget https://github.com/googly-mingto/ML2023HW4/releases/download/data/Dataset.tar.gz.partab\n!wget https://github.com/googly-mingto/ML2023HW4/releases/download/data/Dataset.tar.gz.partac\n!wget https://github.com/googly-mingto/ML2023HW4/releases/download/data/Dataset.tar.gz.partad\n\n!cat Dataset.tar.gz.part* > Dataset.tar.gz\n!rm Dataset.tar.gz.partaa\n!rm Dataset.tar.gz.partab\n!rm Dataset.tar.gz.partac\n!rm Dataset.tar.gz.partad\n# unzip the file\n!tar zxf Dataset.tar.gz\n!rm Dataset.tar.gz","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-06-25T05:11:49.614283Z","iopub.execute_input":"2025-06-25T05:11:49.614933Z","iopub.status.idle":"2025-06-25T05:14:38.158153Z","shell.execute_reply.started":"2025-06-25T05:11:49.614908Z","shell.execute_reply":"2025-06-25T05:14:38.156934Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!tar zxf Dataset.tar.gz","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-25T05:17:39.812394Z","iopub.execute_input":"2025-06-25T05:17:39.812702Z","iopub.status.idle":"2025-06-25T05:17:39.955755Z","shell.execute_reply.started":"2025-06-25T05:17:39.812675Z","shell.execute_reply":"2025-06-25T05:17:39.955107Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nimport torch\nimport random\n\ndef set_seed(seed):\n    np.random.seed(seed) # 设置 NumPy 的随机种子\n    random.seed(seed)  # 设置 Python 内置 random 模块的随机种子\n    torch.manual_seed(seed) # 设置 PyTorch 的 CPU 随机种子\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed(seed)  # 设置当前 GPU 的随机种子\n        torch.cuda.manual_seed_all(seed) # 设置所有 GPU 的随机种子（多卡时）\n    # 如果使用 GPU，额外设置 CUDA 的随机种子，保证 GPU 上的随机操作（如 torch.rand()）可复现。\n    torch.backends.cudnn.benchmark = False\n    # benchmark=False：禁止 CuDNN 自动选择最快的卷积算法（因为不同算法可能导致微小数值差异）。\n    torch.backends.cudnn.deterministic = True\n    # deterministic=True：强制 CuDNN 使用确定性算法，牺牲速度换取严格可复现性。\n\nset_seed(87)\n# 调用后，所有随机操作（如模型初始化、数据加载时的打乱）将基于种子 87 生成，确保多次运行结果一致。\nprint(\"Complete\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-25T05:49:27.856507Z","iopub.execute_input":"2025-06-25T05:49:27.857114Z","iopub.status.idle":"2025-06-25T05:49:27.863724Z","shell.execute_reply.started":"2025-06-25T05:49:27.857085Z","shell.execute_reply":"2025-06-25T05:49:27.862948Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"为什么需要这个类？\n统一输入尺寸：神经网络需要固定长度的输入（比如128帧），但录音长短不一，必须裁剪或填充。\n\n打乱数据：通过 DataLoader 可以随机打乱顺序，避免模型死记硬背。\n\n说话人分类：每个片段对应一个说话人ID，模型的目标是学会区分不同人的声音。","metadata":{}},{"cell_type":"code","source":"import os\nimport json\nimport torch\nimport random\nfrom pathlib import Path\nfrom torch.utils.data import Dataset\nfrom torch.nn.utils.rnn import pad_sequence\n\n\nclass myDataset(Dataset):\n    def __init__(self, data_dir, segment_len=128):\n        self.data_dir = data_dir\n        self.segment_len = segment_len\n    \n        # 有一本\"电话簿\"(mapping.json)，记录每个说话人的名字和对应的ID(比如\"张三→1，李四→2\")\n        mapping_path = Path(data_dir) / \"mapping.json\"\n        mapping = json.load(mapping_path.open())\n        self.speaker2id = mapping[\"speaker2id\"]\n    \n        # 有一张\"录音清单\"(metadata.json)，记录每个录音文件的位置和属于哪个说话人\n        metadata_path = Path(data_dir) / \"metadata.json\"\n        metadata = json.load(open(metadata_path))[\"speakers\"]\n    \n        # 把所有录音文件的路径和对应的说话人ID整理成一个列表(self.data)\n        self.speaker_num = len(metadata.keys())\n        self.data = []\n        for speaker in metadata.keys():\n            for utterances in metadata[speaker]:\n                self.data.append([utterances[\"feature_path\"], self.speaker2id[speaker]])\n    \n    # 有多少条录音\n    def __len__(self):\n        return len(self.data)  # 即所有语音文件的数量\n \n    def __getitem__(self, index):\n        feat_path, speaker = self.data[index]\n        # 加载 Mel 频谱图\n        mel = torch.load(os.path.join(self.data_dir, feat_path))\n        # 长的随机剪一段短的直接整段用(可能会补零，但这里没写)\n        if len(mel) > self.segment_len:\n            start = random.randint(0, len(mel) - self.segment_len)\n            mel = torch.FloatTensor(mel[start:start+self.segment_len])\n        else:\n            mel = torch.FloatTensor(mel)\n        # 将说话人ID转为long类型以便计算loss\n        speaker = torch.FloatTensor([speaker]).long()\n        # 返回 (mel_spectrogram, speaker_id)\n        return mel, speaker\n    \n    # 有多少个不同的说话人\n    def get_speaker_number(self):\n        return self.speaker_num\n\nprint(\"Complete\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-25T05:49:37.430744Z","iopub.execute_input":"2025-06-25T05:49:37.431633Z","iopub.status.idle":"2025-06-25T05:49:37.443095Z","shell.execute_reply.started":"2025-06-25T05:49:37.431599Z","shell.execute_reply":"2025-06-25T05:49:37.442396Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import DataLoader, random_split\nfrom torch.nn.utils.rnn import pad_sequence\n\n\ndef collate_batch(batch):\n\t# Process features within a batch.\n\t\"\"\"Collate a batch of data.\"\"\"\n\tmel, speaker = zip(*batch)\n\t# Because we train the model batch by batch, we need to pad the features in the same batch to make their lengths the same.\n\tmel = pad_sequence(mel, batch_first=True, padding_value=-20)    # pad log 10^(-20) which is very small value.\n\t# mel: (batch size, length, 40)\n\treturn mel, torch.FloatTensor(speaker).long()\n\n\ndef get_dataloader(data_dir, batch_size, n_workers):\n\t\"\"\"Generate dataloader\"\"\"\n\tdataset = myDataset(data_dir)\n\tspeaker_num = dataset.get_speaker_number()\n\t# Split dataset into training dataset and validation dataset\n\ttrainlen = int(0.9 * len(dataset))\n\tlengths = [trainlen, len(dataset) - trainlen]\n\ttrainset, validset = random_split(dataset, lengths)\n\n\ttrain_loader = DataLoader(\n\t\ttrainset,\n\t\tbatch_size=batch_size,\n\t\tshuffle=True,\n\t\tdrop_last=True,\n\t\tnum_workers=n_workers,\n\t\tpin_memory=True,\n\t\tcollate_fn=collate_batch,\n\t)\n\tvalid_loader = DataLoader(\n\t\tvalidset,\n\t\tbatch_size=batch_size,\n\t\tnum_workers=n_workers,\n\t\tdrop_last=True,\n\t\tpin_memory=True,\n\t\tcollate_fn=collate_batch,\n\t)\n\n\treturn train_loader, valid_loader, speaker_num\n\nprint(\"Complete\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-25T05:49:44.650342Z","iopub.execute_input":"2025-06-25T05:49:44.650625Z","iopub.status.idle":"2025-06-25T05:49:44.657454Z","shell.execute_reply.started":"2025-06-25T05:49:44.650605Z","shell.execute_reply":"2025-06-25T05:49:44.656627Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Transformer\n## 一种基于 自注意力机制（Self-Attention） 的深度学习模型架构\n## Transformer 的核心是 自注意力机制（Self-Attention），它可以让模型在处理序列数据（如文本）时，动态地关注输入的不同部分，而不像 RNN/LSTM 那样依赖固定的顺序计算。\n### 输入：一个序列（如句子中的单词）\n### 计算过程：Query, Key, Value（QKV）：每个输入单词会被转换成 3 个向量：Query（查询）：表示当前单词要查询其他单词的重要性。Key（键）：表示当前单词对其他单词的贡献程度。Value（值）：表示当前单词的实际信息。计算注意力分数：计算 Query 和 Key 的点积，得到 注意力分数（表示单词之间的相关性）。用 Softmax 归一化，得到 注意力权重。加权求和：用注意力权重对 Value 加权求和，得到最终的 注意力输出。\n## Transformer 由 编码器（Encoder） 和 解码器（Decoder） 组成\n## Transformer 的关键参数\n### d_model,输入/输出的特征维度（如词向量维度）：通常取 512、768、1024，取决于任务复杂度。更大的 d_model 能存储更多信息，但计算量也更大。\n### nhead,多头注意力的头数（并行计算）:一般取 8~16，需能被 d_model 整除（如 d_model=512, nhead=8 → 每个头维度=512/8=64）\n### num_layers,Encoder/Decoder 的层数:num_layers：简单任务 6 层，大模型（如 GPT-3）可达 96 层。\n### dim_feedforward\t前馈神经网络的隐藏层维度:通常取 4 × d_model（如 d_model=512 → dim_feedforward=2048）。\n## 比 RNN/LSTM 更高效、更强大，适用于多种任务。如果你要实现一个 Transformer，可以使用 PyTorch 的 nn.Transformer 模块，或直接调用 Hugging Face 的现成模型（如 BERT、GPT-2）。","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Classifier(nn.Module):\n\tdef __init__(self, d_model=80, n_spks=600, dropout=0.1):\n\t\tsuper().__init__()\n\n        # 特征预处理 (prenet): 将输入的特征维度转换为模型内部所需的维度。\n\t\tself.prenet = nn.Linear(40, d_model)\n\n        # 序列编码 (encoder_layer / encoder): 使用 Transformer 编码器来捕捉输入序列（语音帧）中的时序依赖和上下文信息。\n\t\tself.encoder_layer = nn.TransformerEncoderLayer(\n\t\t\td_model=d_model, dim_feedforward=256, nhead=2\n\t\t)\n\t\tself.encoder = nn.TransformerEncoder(self.encoder_layer, num_layers=2)\n\n        # 最终预测 (pred_layer): 将编码器输出的序列信息聚合成固定表示，然后映射到最终的说话人分类得分。 \n\t\tself.pred_layer = nn.Sequential(\n\t\t\tnn.Linear(d_model, d_model),\n\t\t\tnn.Sigmoid(),\n\t\t\tnn.Linear(d_model, n_spks),\n\t\t)\n\n\tdef forward(self, mels):\n\t\t\"\"\"\n\t\targs:\n\t\t\tmels: (batch size, length, 40)\n\t\treturn:\n\t\t\tout: (batch size, n_spks)\n\t\t\"\"\"\n\t\t# 将原始的语音特征调整到了你的 Transformer 编码器期望的特征维度 (d_model)\n\t\tout = self.prenet(mels)\n\t\t# 交换张量的维度\n\t\tout = out.permute(1, 0, 2)\n\t\t# 对输入序列的深度特征提取,每经过一层，模型对序列的理解就更深入一些\n\t\tout = self.encoder_layer(out)\n\t\t# 交换张量 out 的前两个维度（索引为 0 和 1 的维度）和上面那个不一样,要区分\n\t\tout = out.transpose(0, 1)\n\t\t# 算张量 out 沿着第二个维度（索引为 1 的维度）的平均值\n\t\tstats = out.mean(dim=1)\n\n\t\t# 输入传递给模型中的一个预测层 \n\t\tout = self.pred_layer(stats)\n\t\treturn out\n\nprint(\"Complete\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-25T05:49:52.752660Z","iopub.execute_input":"2025-06-25T05:49:52.753324Z","iopub.status.idle":"2025-06-25T05:49:52.759912Z","shell.execute_reply.started":"2025-06-25T05:49:52.753297Z","shell.execute_reply":"2025-06-25T05:49:52.758962Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import math\n\nimport torch\nfrom torch.optim import Optimizer\nfrom torch.optim.lr_scheduler import LambdaLR\n\n\ndef get_cosine_schedule_with_warmup(\n\toptimizer: Optimizer, # 要调度的优化器\n\tnum_warmup_steps: int, # 预热步数\n\tnum_training_steps: int, # 总训练步数\n\tnum_cycles: float = 0.5, # 余弦周期数\n\tlast_epoch: int = -1, # 恢复训练时指定上次的epoch\n):\n\t\n\tdef lr_lambda(current_step):\n\t\t# Warmup\n\t\tif current_step < num_warmup_steps:\n\t\t\treturn float(current_step) / float(max(1, num_warmup_steps))\n\t\t# decadence\n\t\tprogress = float(current_step - num_warmup_steps) / float(\n\t\t\tmax(1, num_training_steps - num_warmup_steps)\n\t\t)\n\t\treturn max(\n\t\t\t0.0, 0.5 * (1.0 + math.cos(math.pi * float(num_cycles) * 2.0 * progress))\n\t\t)\n\n\treturn LambdaLR(optimizer, lr_lambda, last_epoch)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-25T05:56:11.333089Z","iopub.execute_input":"2025-06-25T05:56:11.334129Z","iopub.status.idle":"2025-06-25T05:56:11.340581Z","shell.execute_reply.started":"2025-06-25T05:56:11.334088Z","shell.execute_reply":"2025-06-25T05:56:11.339670Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\n\n\ndef model_fn(batch, model, criterion, device):\n\n\tmels, labels = batch\n\tmels = mels.to(device)\n\tlabels = labels.to(device)\n    \n    # 将输入数据传递给模型，并获取模型的输出。\n\touts = model(mels)\n    # 损失函数，计算loss代表了模型当前表现得有多‘差’\n    # loss 的值越小，表示模型的预测越接近真实情况，模型表现得越好。\n    # loss 的值越大，表示模型的预测与真实情况偏差越大，模型表现得越差。\n\tloss = criterion(outs, labels)\n\n\t# 对于每个输入样本，把该类别的索引作为最终的预测结果\n\tpreds = outs.argmax(1)\n    # 计算模型预测正确的样本占总样本数的比例\n\taccuracy = torch.mean((preds == labels).float())\n\n\treturn loss, accuracy","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-25T05:56:16.883042Z","iopub.execute_input":"2025-06-25T05:56:16.883304Z","iopub.status.idle":"2025-06-25T05:56:16.888081Z","shell.execute_reply.started":"2025-06-25T05:56:16.883287Z","shell.execute_reply":"2025-06-25T05:56:16.887243Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from tqdm import tqdm\nimport torch\n\n\ndef valid(dataloader, model, criterion, device): \n\n    # 切换到评估模式\n\tmodel.eval()\n    \n\trunning_loss = 0.0\n\trunning_accuracy = 0.0\n\n    # 创建进度条\n\tpbar = tqdm(total=len(dataloader.dataset), ncols=0, desc=\"Valid\", unit=\" uttr\")\n\n\tfor i, batch in enumerate(dataloader):\n\t\twith torch.no_grad():\n\t\t\tloss, accuracy = model_fn(batch, model, criterion, device)\n\t\t\trunning_loss += loss.item()\n\t\t\trunning_accuracy += accuracy.item()\n        # 更新进度条的显示\n\t\tpbar.update(dataloader.batch_size)\n        # 进度条的右侧实时显示额外信息\n\t\tpbar.set_postfix(\n\t\t\tloss=f\"{running_loss / (i+1):.2f}\",\n\t\t\taccuracy=f\"{running_accuracy / (i+1):.2f}\",\n\t\t)\n    #关闭并清理进度条\n\tpbar.close()\n    #切换到训练模式\n\tmodel.train()\n    \n    # 计算并返回模型在整个验证集上的平均准确率\n\treturn running_accuracy / len(dataloader)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-25T05:56:19.177408Z","iopub.execute_input":"2025-06-25T05:56:19.178185Z","iopub.status.idle":"2025-06-25T05:56:19.183880Z","shell.execute_reply.started":"2025-06-25T05:56:19.178149Z","shell.execute_reply":"2025-06-25T05:56:19.183265Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from tqdm import tqdm\n\nimport torch\nimport torch.nn as nn\nfrom torch.optim import AdamW\n# 训练像 Transformer 这样的大型模型时，AdamW 通常是更推荐的选择。带解耦权重衰减的 Adam。\n# 这是因为模型的参数量巨大，L2 正则化（权重衰减）对于防止过拟合至关重要。\nfrom torch.utils.data import DataLoader, random_split\n\n\ndef parse_args():\n\n\tconfig = {\n\t\t\"data_dir\": \"./Dataset\",\n\t\t\"save_path\": \"model.ckpt\",\n\t\t\"batch_size\": 256,\n\t\t\"n_workers\": 16,\n\t\t\"valid_steps\": 1000,\n\t\t\"warmup_steps\": 2000,\n\t\t\"save_steps\": 5000,\n\t\t\"total_steps\": 20000,\n\t}\n\n\treturn config\n\n\ndef main(\n\tdata_dir,\n\tsave_path,\n\tbatch_size,\n\tn_workers,\n\tvalid_steps,\n\twarmup_steps,\n\ttotal_steps,\n\tsave_steps,\n):\n\n\tdevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\tprint(f\"[Info]: Use {device} now!\")\n\n\ttrain_loader, valid_loader, speaker_num = get_dataloader(data_dir, batch_size, n_workers)\n\ttrain_iterator = iter(train_loader)\n\tprint(f\"[Info]: Finish loading data!\",flush = True)\n\n\tmodel = Classifier(n_spks=speaker_num).to(device)\n\tcriterion = nn.CrossEntropyLoss()\n\toptimizer = AdamW(model.parameters(), lr=1e-3)\n\tscheduler = get_cosine_schedule_with_warmup(optimizer, warmup_steps, total_steps)\n\tprint(f\"[Info]: Finish creating model!\",flush = True)\n\n\tbest_accuracy = -1.0\n    # 存储模型参数（权重和偏置）的副本\n\tbest_state_dict = None\n\n\tpbar = tqdm(total=valid_steps, ncols=0, desc=\"Train\", unit=\" step\")\n\n\tfor step in range(total_steps):\n\n\t\ttry:\n\t\t\tbatch = next(train_iterator)\n\t\texcept StopIteration:\n\t\t\ttrain_iterator = iter(train_loader)\n\t\t\tbatch = next(train_iterator)\n\n\t\tloss, accuracy = model_fn(batch, model, criterion, device)\n\t\tbatch_loss = loss.item()\n\t\tbatch_accuracy = accuracy.item()\n\n\t\t\n\t\tloss.backward()\n\t\toptimizer.step()\n\t\tscheduler.step()\n\t\toptimizer.zero_grad()\n\n\t\n\t\tpbar.update()\n\t\tpbar.set_postfix(\n\t\t\tloss=f\"{batch_loss:.2f}\",\n\t\t\taccuracy=f\"{batch_accuracy:.2f}\",\n\t\t\tstep=step + 1,\n\t\t)\n\n\n\t\tif (step + 1) % valid_steps == 0:\n\t\t\tpbar.close()\n\n\t\t\tvalid_accuracy = valid(valid_loader, model, criterion, device)\n\n\n\t\t\tif valid_accuracy > best_accuracy:\n\t\t\t\tbest_accuracy = valid_accuracy\n\t\t\t\tbest_state_dict = model.state_dict()\n\n\t\t\tpbar = tqdm(total=valid_steps, ncols=0, desc=\"Train\", unit=\" step\")\n\n\n\t\tif (step + 1) % save_steps == 0 and best_state_dict is not None:\n\t\t\ttorch.save(best_state_dict, save_path)\n\t\t\tpbar.write(f\"Step {step + 1}, best model saved. (accuracy={best_accuracy:.4f})\")\n\n\tpbar.close()\n\n\nif __name__ == \"__main__\":\n\tmain(**parse_args())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-25T07:16:22.245452Z","iopub.execute_input":"2025-06-25T07:16:22.246357Z","iopub.status.idle":"2025-06-25T07:34:28.653007Z","shell.execute_reply.started":"2025-06-25T07:16:22.246320Z","shell.execute_reply":"2025-06-25T07:34:28.652089Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os # 提供了与操作系统交互的功能，例如路径拼接\nimport json # 用于处理 JSON 格式的数据\nimport torch # 这是深度学习框架，用于张量操作等\nfrom pathlib import Path # 从 pathlib 模块导入 Path 类，它提供了面向对象的文件系统路径操作，比 os.path 更现代和方便。\nfrom torch.utils.data import Dataset #  从 PyTorch 的 utils.data 模块导入 Dataset 基类。所有自定义的数据集类都需要继承它。\n\n\nclass InferenceDataset(Dataset):\n\tdef __init__(self, data_dir):\n\t\t# Path(data_dir) 将 data_dir 字符串转换为一个 Path 对象，然后 / 运算符用于路径拼接，这比 os.path.join 更直观和安全。\n        testdata_path = Path(data_dir) / \"testdata.json\" \n\t\t# 打开 testdata_path 指向的 JSON 文件（testdata_path.open() 返回一个文件对象），然后使用 json.load() 函数加载并解析 JSON 数据到 metadata 字典中。\n        metadata = json.load(testdata_path.open())\n\t\t# 特征文件的根目录\n        self.data_dir = data_dir\n        # 从加载的 metadata 字典中，提取键为 \"utterances\" 的值，并将其存储为实例的属性 self.data。\n        # \"utterances\" 应该是一个列表，其中每个元素代表一个要进行推理的语音片段（或称为“话语”），并包含其特征文件的路径等信息。\n\t\tself.data = metadata[\"utterances\"]\n\n\tdef __len__(self):\n        # 在数据集对象上调用 len() 函数时（例如 len(dataset_instance)），这个方法会被调用。\n\t\treturn len(self.data)\n\n\tdef __getitem__(self, index):\n        # 这是 Dataset 类另一个必需的方法。它使得数据集对象可以像列表一样通过索引来访问（例如 dataset_instance[0]）。当给定一个 index 时，这个方法应该返回对应索引的数据样本。\n\t\tutterance = self.data[index]\n        # 从当前 utterance 字典中提取键为 \"feature_path\" 的值。这应该是相对于 self.data_dir 的特征文件的路径。\n\t\tfeat_path = utterance[\"feature_path\"]\n        # 指定的路径加载一个 PyTorch 张量\n\t\tmel = torch.load(os.path.join(self.data_dir, feat_path))\n\n\t\treturn feat_path, mel\n\n# 这个函数用于将多个单独的数据样本（由 __getitem__ 返回）组合成一个批次（batch）\ndef inference_collate_batch(batch):\n\t# feat_paths 将是一个包含所有文件路径的元组（(path1, path2, ...)），mels 将是一个包含所有梅尔频谱图张量的元组（(mel1, mel2, ...)）。\n\t# *batch: 将 batch 列表解包，例如如果 batch 是 [(path1, mel1), (path2, mel2)]，那么 *batch 就会变成 (path1, mel1), (path2, mel2)。\n    # zip(...): zip 函数将这些独立的元组重新组合。它会把所有第一个元素打包成一个元组，所有第二个元素打包成另一个元组。\n    feat_paths, mels = zip(*batch)\n\n\treturn feat_paths, torch.stack(mels)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-25T07:34:40.585464Z","iopub.execute_input":"2025-06-25T07:34:40.586489Z","iopub.status.idle":"2025-06-25T07:34:40.594665Z","shell.execute_reply.started":"2025-06-25T07:34:40.586450Z","shell.execute_reply":"2025-06-25T07:34:40.593996Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import json\nimport csv # 用于写入 CSV 文件（保存推理结果）\nfrom pathlib import Path\nfrom tqdm.notebook import tqdm # 从tqdm.notebook导入 tqdm，这是一个进度条库，用于在循环中显示进度，特别适合 Jupyter Notebook 环境。\n\nimport torch\nfrom torch.utils.data import DataLoader\n\ndef parse_args():\n\t# 配置参数\n\tconfig = {\n\t\t\"data_dir\": \"./Dataset\",\n\t\t\"model_path\": \"./model.ckpt\",\n\t\t\"output_path\": \"./output.csv\",\n\t}\n\n\treturn config\n\n\ndef main(\n\tdata_dir,\n\tmodel_path,\n\toutput_path,\n):\n    # 语法糖\n\tdevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\tprint(f\"[Info]: Use {device} now!\")\n\n\tmapping_path = Path(data_dir) / \"mapping.json\"\n\tmapping = json.load(mapping_path.open())\n\n\tdataset = InferenceDataset(data_dir)\n\tdataloader = DataLoader(\n\t\tdataset,\n\t\tbatch_size=1,\n\t\tshuffle=False,\n\t\tdrop_last=False,\n        #  使用 8 个子进程来并行加载数据。这可以加速数据加载过程。\n\t\tnum_workers=8,\n\t\tcollate_fn=inference_collate_batch,\n\t)\n\tprint(f\"[Info]: Finish loading data!\",flush = True)\n\n\tspeaker_num = len(mapping[\"id2speaker\"])\n\tmodel = Classifier(n_spks=speaker_num).to(device)\n\tmodel.load_state_dict(torch.load(model_path))\n\tmodel.eval()\n\tprint(f\"[Info]: Finish creating model!\",flush = True)\n\n\tresults = [[\"Id\", \"Category\"]]\n\tfor feat_paths, mels in tqdm(dataloader):\n        # PyTorch 将不会计算梯度。这对于推理阶段非常重要，因为推理不需要反向传播来更新权重，禁用梯度计算可以节省内存并加速推理。\n\t\twith torch.no_grad():\n\t\t\tmels = mels.to(device)\n\t\t\touts = model(mels)\n\t\t\tpreds = outs.argmax(1).cpu().numpy()\n\t\t\tfor feat_path, pred in zip(feat_paths, preds):\n\t\t\t\tresults.append([feat_path, mapping[\"id2speaker\"][str(pred)]])\n\n    # 以写入模式 ('w') 打开指定的 output_path 文件。newline='' 对于 CSV 文件很重要，可以防止写入额外的空行。\n\twith open(output_path, 'w', newline='') as csvfile:\n\t\twriter = csv.writer(csvfile)\n\t\twriter.writerows(results)\n\n# 这是一个标准的 Python 惯用法，确保 main() 函数只在脚本作为主程序直接运行时才执行，而不是在被其他模块导入时执行。\nif __name__ == \"__main__\":\n\tmain(**parse_args())\n    # 调用 main 函数。**parse_args() 会解包 parse_args() 返回的字典，将其中的键值对作为关键字参数传递给 main 函数，\n    # 例如 main(data_dir=\"./Dataset\", model_path=\"./model.ckpt\", output_path=\"./output.csv\")。","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-25T07:34:43.135372Z","iopub.execute_input":"2025-06-25T07:34:43.136058Z","iopub.status.idle":"2025-06-25T07:34:59.678803Z","shell.execute_reply.started":"2025-06-25T07:34:43.136036Z","shell.execute_reply":"2025-06-25T07:34:59.678095Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 本来想选择openai/whisper-large-v3再优化一下跑一跑,但是显存不够没跑起来","metadata":{}},{"cell_type":"code","source":"!pip install -U transformers","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-25T03:49:51.014465Z","iopub.execute_input":"2025-06-25T03:49:51.014778Z","iopub.status.idle":"2025-06-25T03:50:09.007494Z","shell.execute_reply.started":"2025-06-25T03:49:51.014746Z","shell.execute_reply":"2025-06-25T03:50:09.006179Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Use a pipeline as a high-level helper\nfrom transformers import pipeline\n\npipe = pipeline(\"automatic-speech-recognition\", model=\"openai/whisper-large-v3\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-25T03:55:21.291917Z","iopub.execute_input":"2025-06-25T03:55:21.292122Z","iopub.status.idle":"2025-06-25T03:56:38.362729Z","shell.execute_reply.started":"2025-06-25T03:55:21.292105Z","shell.execute_reply":"2025-06-25T03:56:38.362068Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Load model directly\nfrom transformers import AutoProcessor, AutoModelForSpeechSeq2Seq\n\nprocessor = AutoProcessor.from_pretrained(\"openai/whisper-large-v3\")\nmodel = AutoModelForSpeechSeq2Seq.from_pretrained(\"openai/whisper-large-v3\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-25T03:56:47.942868Z","iopub.execute_input":"2025-06-25T03:56:47.943542Z","iopub.status.idle":"2025-06-25T03:56:53.875408Z","shell.execute_reply.started":"2025-06-25T03:56:47.943510Z","shell.execute_reply":"2025-06-25T03:56:53.874823Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport json\nimport torch\nimport numpy as np\nfrom torch.utils.data import Dataset, DataLoader\nfrom transformers import WhisperFeatureExtractor, WhisperForAudioClassification\nfrom torch.optim import AdamW\nfrom tqdm import tqdm\n\n# 1. 数据加载\nclass SpeakerDataset(Dataset):\n    def __init__(self, data_dir):\n        self.data_dir = data_dir\n        with open(f\"{data_dir}/mapping.json\") as f:\n            self.speaker2id = json.load(f)[\"speaker2id\"]\n        with open(f\"{data_dir}/metadata.json\") as f:\n            metadata = json.load(f)\n        self.data = [\n            (utt[\"feature_path\"], self.speaker2id[spk]) \n            for spk in metadata[\"speakers\"] \n            for utt in metadata[\"speakers\"][spk]\n        ]\n        self.feature_extractor = WhisperFeatureExtractor.from_pretrained(\"openai/whisper-large-v3\")\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        path, speaker = self.data[idx]\n        mel = torch.load(f\"{self.data_dir}/{path}\").numpy()\n        inputs = self.feature_extractor(mel, sampling_rate=16000, return_tensors=\"pt\")\n        return inputs.input_features[0], torch.tensor(speaker)\n\n# 2. 数据处理\ndef collate_fn(batch):\n    features, labels = zip(*batch)\n    features = torch.stack(features)\n    return features, torch.stack(labels)\n\n# 3. 模型定义\nclass SpeakerClassifier(torch.nn.Module):\n    def __init__(self, num_speakers):\n        super().__init__()\n        self.whisper = WhisperForAudioClassification.from_pretrained(\n            \"openai/whisper-large-v3\",\n            num_labels=num_speakers\n        )\n        # 冻结大部分层\n        for param in self.whisper.parameters():\n            param.requires_grad = False\n        # 解冻分类层\n        for param in self.whisper.classifier.parameters():\n            param.requires_grad = True\n\n    def forward(self, x):\n        return self.whisper(x).logits\n\n# 4. 训练函数\ndef train(data_dir, epochs=5, batch_size=16):\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    \n    # 加载数据\n    dataset = SpeakerDataset(data_dir)\n    train_size = int(0.9 * len(dataset))\n    train_set, val_set = torch.utils.data.random_split(dataset, [train_size, len(dataset)-train_size])\n    \n    train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n    val_loader = DataLoader(val_set, batch_size=batch_size, collate_fn=collate_fn)\n\n    # 初始化模型\n    model = SpeakerClassifier(len(dataset.speaker2id)).to(device)\n    optimizer = AdamW(model.parameters(), lr=1e-4)\n    criterion = torch.nn.CrossEntropyLoss()\n\n    # 训练循环\n    for epoch in range(epochs):\n        model.train()\n        for features, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}\"):\n            features, labels = features.to(device), labels.to(device)\n            \n            outputs = model(features)\n            loss = criterion(outputs, labels)\n            \n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n\n        # 验证\n        model.eval()\n        correct = 0\n        total = 0\n        with torch.no_grad():\n            for features, labels in val_loader:\n                features, labels = features.to(device), labels.to(device)\n                outputs = model(features)\n                _, predicted = torch.max(outputs.data, 1)\n                total += labels.size(0)\n                correct += (predicted == labels).sum().item()\n        \n        print(f\"验证准确率: {correct/total:.4f}\")\n\n    return model\n\n# 5. 预测函数\ndef predict(model, audio_features):\n    device = next(model.parameters()).device\n    model.eval()\n    with torch.no_grad():\n        inputs = model.feature_extractor(audio_features, return_tensors=\"pt\").input_features.to(device)\n        outputs = model(inputs)\n        _, predicted = torch.max(outputs, 1)\n    return predicted.item()\n\n# 使用示例\nif __name__ == \"__main__\":\n    # 训练模型\n    model = train(\"Dataset\")\n    \n    # 保存模型\n    torch.save(model.state_dict(), \"speaker_classifier.pt\")\n    \n    # 加载测试数据示例\n    test_mel = torch.load(\"Dataset/features/test_sample.pt\")  # 替换为你的测试数据\n    predicted_id = predict(model, test_mel.numpy())\n    print(f\"预测说话人ID: {predicted_id}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-25T05:36:56.970981Z","iopub.execute_input":"2025-06-25T05:36:56.971544Z","execution_failed":"2025-06-25T05:41:12.123Z"}},"outputs":[],"execution_count":null}]}