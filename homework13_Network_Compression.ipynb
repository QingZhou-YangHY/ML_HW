{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Network Compression:smaller model less parameters\n",
    "理由：resource-constrained environments\n",
    "为什么不上云? Lower latency,Privacy,etc.\n",
    "这里只考虑:软体压缩 不考虑硬体加速\n",
    "Network Pruning:Network can be pruned   1989年就提出了Optimal Brain Damage 这篇paper\n",
    "framework:Pre-trained Network很大,Evaluate the importance of a weight(absolute values,life long...)/neuron(the number of times it wasn't zero on a given data set...),之后Remove就得到了比较小的Network,这样accuracy就会掉一点,然后我们你需要Fine-tune一下让他的accuracy上升回来.还可以重新评估一次,再Remove,重新Fine-tune.  tips:一次只剪掉一点\n",
    "Network Pruning - Practical Issue\n",
    "Weight pruning:剪掉之后的network是不规则的,导致hard to implement,hard to speedup,不好实做.因为input和output不好写vector,GPU加速也不好加,因为矩阵的乘法不容易.所以Prune掉的weight直接补零.这样的问题是没有真正地把Network变小,因为这样参数还是存在,只不过为零.多数情况下都是变慢的.\n",
    "Neuron pruning:network剪完后是规则的.\n",
    "\n",
    "大的network比较好trian(Lottery Ticket Hypothesis)\n",
    "大的network可以看成许多小的network,只要有一个小的network,大的network就成功.(大乐透假说)\n",
    "直接训练小的,train不起来.\n",
    "解构大乐透.不同的策略是不同的效果.不改变参数的正负号是关键,绝对值不重要.随机初始化一下得到了好的结果,不用train,直接prune就可以(Weight Agnostic Neural Networks也有类似的结论)\n",
    "\n",
    "Knowledge Distillation\n",
    "先训练一个大的Network叫Teacher Net,根据这个训练出来的小的叫Student Net(修剪).\n",
    "Student Net的输出要接近 Teacher Net的输出(Learning target).不直接trian小的network的原因和上面一样.因为往往效果不怎么样.Student Net可能没有看过input,但也会.\n",
    "Teacher Net 可以是多个Network的Ensemble.Ensemble:Average many model.\n",
    "\n",
    "Temperature for softmax\n",
    "作用:让比较集中的分布变得比较平滑.让学生学到数据之间的关系,学的比较好.\n",
    "Parameter Quantization\n",
    "1.用少的空间储存比较多的参数\n",
    "2.把相近的数值分群,然后把每个群里面取出平均值.\n",
    "3.常出现的用少的bit描述,罕见的用比较多的bit描述 e.g.Huffman encoding\n",
    "Binary Weights(效果居然比较好,还可以达到防止overfitting的作用)\n",
    "\n",
    "\n",
    "Architecture Design\n",
    "*Depthwise Separable Convolution*\n",
    "Review: Standard CNN\n",
    "Input:feature map\n",
    "filter的数量 = output的feature map\n",
    "\n",
    "一般的CNN的Input和output的channel是不一样的,但是在Depthwise Convolution里面是一样的\n",
    "1.Depthwise Convolution 同一个channel内部的关系\n",
    "Filter number = Input channel number 每个filter只负责一个Input channel 这里的filter没有厚度\n",
    "\n",
    "2.Pointwise Convolution 输入和输出之间的channel可以不一样,只需要考虑channel之间的关系\n",
    "Kernel Size 都是1 x 1\n",
    "\n",
    "Low rank approximation\n",
    "把一层network拆成两层network,对参数的需求减小了.但是W中的参数受限制\n",
    "\n",
    "Dynamic Computation\n",
    "前几个方法是把Network变小,但再Dynamic Computation可以自由调整运算量.\n",
    "让network自由调整深度.训练的时候把每个extra layer的output和ground truth越接近越好,把所有的output的ground truth的距离加起来得到L,minimize L.\n",
    "\n",
    "让network自由决定宽度.和上面的一样.\n",
    "\n",
    "Computation based on Sample Difficulty\n",
    "\n",
    "这几个压缩是可以一起用的,不互斥."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-07-03T02:57:38.156488Z",
     "iopub.status.busy": "2025-07-03T02:57:38.156258Z",
     "iopub.status.idle": "2025-07-03T02:57:45.899108Z",
     "shell.execute_reply": "2025-07-03T02:57:45.898502Z",
     "shell.execute_reply.started": "2025-07-03T02:57:38.156466Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Import some useful packages for this homework\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import os\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "from torch.utils.data import ConcatDataset, DataLoader, Subset, Dataset # \"ConcatDataset\" and \"Subset\" are possibly useful\n",
    "from torchvision.datasets import DatasetFolder, VisionDataset\n",
    "from torchsummary import summary\n",
    "from tqdm.auto import tqdm\n",
    "import random\n",
    "\n",
    "# !nvidia-smi # list your current GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-03T04:12:01.292732Z",
     "iopub.status.busy": "2025-07-03T04:12:01.292243Z",
     "iopub.status.idle": "2025-07-03T04:12:01.296914Z",
     "shell.execute_reply": "2025-07-03T04:12:01.296121Z",
     "shell.execute_reply.started": "2025-07-03T04:12:01.292711Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "cfg = {\n",
    "    'dataset_root': '/kaggle/input/homework13-dataset/Food-11',\n",
    "    'save_dir': '/kaggle/working/',\n",
    "    'exp_name': \"simple_baseline\",\n",
    "    'batch_size': 64,\n",
    "    'lr': 3e-4,\n",
    "    'seed': 20220013,\n",
    "    'loss_fn_type': 'KD', # simple baseline: CE, medium baseline: KD. See the Knowledge_Distillation part for more information.\n",
    "    'weight_decay': 1e-5,\n",
    "    'grad_norm_max': 10,\n",
    "    'n_epochs': 3, # train more steps to pass the medium baseline.\n",
    "    'patience': 300,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-03T04:12:31.912805Z",
     "iopub.status.busy": "2025-07-03T04:12:31.912583Z",
     "iopub.status.idle": "2025-07-03T04:12:31.920468Z",
     "shell.execute_reply": "2025-07-03T04:12:31.919773Z",
     "shell.execute_reply.started": "2025-07-03T04:12:31.912791Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'dataset_root': '/kaggle/input/homework13-dataset/Food-11', 'save_dir': '/kaggle/working/', 'exp_name': 'simple_baseline', 'batch_size': 64, 'lr': 0.0003, 'seed': 20220013, 'loss_fn_type': 'KD', 'weight_decay': 1e-05, 'grad_norm_max': 10, 'n_epochs': 3, 'patience': 300}\n"
     ]
    }
   ],
   "source": [
    "myseed = cfg['seed']  # set a random seed for reproducibility\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "np.random.seed(myseed)\n",
    "torch.manual_seed(myseed)\n",
    "random.seed(myseed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(myseed)\n",
    "\n",
    "save_path = os.path.join(cfg['save_dir'], cfg['exp_name']) # create saving directory\n",
    "os.makedirs(save_path, exist_ok=True)\n",
    "\n",
    "# define simple logging functionality\n",
    "log_fw = open(f\"{save_path}/log.txt\", 'w') # open log file to save log outputs\n",
    "def log(text):     # define a logging function to trace the training process\n",
    "    print(text)\n",
    "    log_fw.write(str(text)+'\\n')\n",
    "    log_fw.flush()\n",
    "\n",
    "log(cfg)  # log your configs to the log file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-03T04:12:34.323086Z",
     "iopub.status.busy": "2025-07-03T04:12:34.322840Z",
     "iopub.status.idle": "2025-07-03T04:12:40.388425Z",
     "shell.execute_reply": "2025-07-03T04:12:40.387634Z",
     "shell.execute_reply.started": "2025-07-03T04:12:34.323070Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/homework13-dataset/Food-11: 1 files.\n",
      "/kaggle/input/homework13-dataset/Food-11/validation: 4432 files.\n",
      "/kaggle/input/homework13-dataset/Food-11/training: 9993 files.\n",
      "/kaggle/input/homework13-dataset/Food-11/evaluation: 2218 files.\n"
     ]
    }
   ],
   "source": [
    "for dirname, _, filenames in os.walk('/kaggle/input/homework13-dataset/Food-11'):\n",
    "    if len(filenames) > 0:\n",
    "        print(f\"{dirname}: {len(filenames)} files.\") # Show the file amounts in each split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-03T04:12:43.214227Z",
     "iopub.status.busy": "2025-07-03T04:12:43.213956Z",
     "iopub.status.idle": "2025-07-03T04:12:43.220445Z",
     "shell.execute_reply": "2025-07-03T04:12:43.219650Z",
     "shell.execute_reply.started": "2025-07-03T04:12:43.214206Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "# define training/testing transforms\n",
    "test_tfm = transforms.Compose([\n",
    "    # It is not encouraged to modify this part if you are using the provided teacher model. This transform is stardard and good enough for testing.\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    normalize,\n",
    "])\n",
    "\n",
    "train_tfm = transforms.Compose([\n",
    "    transforms.Resize(256),          # 先缩放到256像素\n",
    "    \n",
    "    # 增强的数据增强组合\n",
    "    transforms.RandomResizedCrop(224, scale=(0.8, 1.0)),  # 随机缩放裁剪(保持224大小)\n",
    "    transforms.RandomHorizontalFlip(p=0.5),               # 50%概率水平翻转\n",
    "    transforms.ColorJitter(\n",
    "        brightness=0.2,             # 亮度调整\n",
    "        contrast=0.2,               # 对比度调整\n",
    "        saturation=0.2,             # 饱和度调整\n",
    "        hue=0.1                     # 色相微调\n",
    "    ),\n",
    "    transforms.RandomRotation(15),   # ±15度随机旋转\n",
    "    \n",
    "    transforms.ToTensor(),           # 转为张量\n",
    "    normalize,                      # 标准化\n",
    "    \n",
    "    # 可选：随机擦除(模拟遮挡)\n",
    "    transforms.RandomErasing(\n",
    "        p=0.5,                      # 50%概率应用\n",
    "        scale=(0.02, 0.2),          # 擦除区域面积比例\n",
    "        ratio=(0.3, 3.3),           # 宽高比范围\n",
    "        value='random'              # 随机值填充\n",
    "    )\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-03T04:12:46.801711Z",
     "iopub.status.busy": "2025-07-03T04:12:46.801083Z",
     "iopub.status.idle": "2025-07-03T04:12:46.806968Z",
     "shell.execute_reply": "2025-07-03T04:12:46.806337Z",
     "shell.execute_reply.started": "2025-07-03T04:12:46.801689Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class FoodDataset(Dataset):\n",
    "    def __init__(self, path, tfm=test_tfm, files = None):\n",
    "        super().__init__()\n",
    "        self.path = path\n",
    "        self.files = sorted([os.path.join(path,x) for x in os.listdir(path) if x.endswith(\".jpg\")])\n",
    "        if files != None:\n",
    "            self.files = files\n",
    "        print(f\"One {path} sample\",self.files[0])\n",
    "        self.transform = tfm\n",
    "  \n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "  \n",
    "    def __getitem__(self,idx):\n",
    "        fname = self.files[idx]\n",
    "        im = Image.open(fname)\n",
    "        im = self.transform(im)\n",
    "        try:\n",
    "            label = int(fname.split(\"/\")[-1].split(\"_\")[0])\n",
    "        except:\n",
    "            label = -1 # test has no label\n",
    "        return im,label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-03T04:12:48.915310Z",
     "iopub.status.busy": "2025-07-03T04:12:48.914654Z",
     "iopub.status.idle": "2025-07-03T04:12:48.940413Z",
     "shell.execute_reply": "2025-07-03T04:12:48.939864Z",
     "shell.execute_reply.started": "2025-07-03T04:12:48.915287Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One /kaggle/input/homework13-dataset/Food-11/training sample /kaggle/input/homework13-dataset/Food-11/training/0_0.jpg\n",
      "One /kaggle/input/homework13-dataset/Food-11/validation sample /kaggle/input/homework13-dataset/Food-11/validation/0_0.jpg\n"
     ]
    }
   ],
   "source": [
    "# Form train/valid dataloaders\n",
    "train_set = FoodDataset(os.path.join(cfg['dataset_root'],\"training\"), tfm=train_tfm)\n",
    "train_loader = DataLoader(train_set, batch_size=cfg['batch_size'], shuffle=True, num_workers=0, pin_memory=True)\n",
    "\n",
    "valid_set = FoodDataset(os.path.join(cfg['dataset_root'], \"validation\"), tfm=test_tfm)\n",
    "valid_loader = DataLoader(valid_set, batch_size=cfg['batch_size'], shuffle=False, num_workers=0, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-03T04:12:50.646196Z",
     "iopub.status.busy": "2025-07-03T04:12:50.645651Z",
     "iopub.status.idle": "2025-07-03T04:12:50.656239Z",
     "shell.execute_reply": "2025-07-03T04:12:50.655422Z",
     "shell.execute_reply.started": "2025-07-03T04:12:50.646176Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def dwpw_conv(in_channels, out_channels, kernel_size, stride=1, padding=0):\n",
    "    return nn.Sequential(\n",
    "        # 深度可分离卷积 = 深度卷积 + 点卷积\n",
    "        nn.Conv2d(in_channels, in_channels, kernel_size, \n",
    "                 stride=stride, padding=padding, groups=in_channels, bias=False),\n",
    "        nn.BatchNorm2d(in_channels),\n",
    "        nn.ReLU(inplace=True),\n",
    "        \n",
    "        # 点卷积(1x1卷积)\n",
    "        nn.Conv2d(in_channels, out_channels, 1, bias=False),\n",
    "        nn.BatchNorm2d(out_channels),\n",
    "        nn.ReLU(inplace=True)\n",
    "    )\n",
    "\n",
    "class StudentNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.cnn = nn.Sequential(\n",
    "            # 初始卷积层(保持较高分辨率)\n",
    "            nn.Conv2d(3, 8, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(8),\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            # 阶段1: 深度可分离卷积\n",
    "            dwpw_conv(8, 16, 3, stride=1, padding=1),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            \n",
    "            # 阶段2: 增加通道数\n",
    "            dwpw_conv(16, 32, 3, stride=1, padding=1),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            \n",
    "            # 阶段3: 进一步提取特征\n",
    "            dwpw_conv(32, 64, 3, stride=1, padding=1),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            \n",
    "            # 阶段4: 最终特征提取\n",
    "            dwpw_conv(64, 128, 3, stride=1, padding=1),\n",
    "            \n",
    "            # 全局平均池化适应不同输入尺寸\n",
    "            nn.AdaptiveAvgPool2d((1, 1)),\n",
    "        )\n",
    "        \n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Dropout(0.2),  # 添加dropout防止过拟合\n",
    "            nn.Linear(128, 11)\n",
    "        )\n",
    "        \n",
    "        # 权重初始化\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.cnn(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        return self.fc(x)\n",
    "\n",
    "def get_student_model():\n",
    "    return StudentNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-03T04:12:52.771126Z",
     "iopub.status.busy": "2025-07-03T04:12:52.770882Z",
     "iopub.status.idle": "2025-07-03T04:12:52.820554Z",
     "shell.execute_reply": "2025-07-03T04:12:52.819458Z",
     "shell.execute_reply.started": "2025-07-03T04:12:52.771108Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1          [-1, 8, 224, 224]             216\n",
      "       BatchNorm2d-2          [-1, 8, 224, 224]              16\n",
      "              ReLU-3          [-1, 8, 224, 224]               0\n",
      "            Conv2d-4          [-1, 8, 224, 224]              72\n",
      "       BatchNorm2d-5          [-1, 8, 224, 224]              16\n",
      "              ReLU-6          [-1, 8, 224, 224]               0\n",
      "            Conv2d-7         [-1, 16, 224, 224]             128\n",
      "       BatchNorm2d-8         [-1, 16, 224, 224]              32\n",
      "              ReLU-9         [-1, 16, 224, 224]               0\n",
      "        MaxPool2d-10         [-1, 16, 112, 112]               0\n",
      "           Conv2d-11         [-1, 16, 112, 112]             144\n",
      "      BatchNorm2d-12         [-1, 16, 112, 112]              32\n",
      "             ReLU-13         [-1, 16, 112, 112]               0\n",
      "           Conv2d-14         [-1, 32, 112, 112]             512\n",
      "      BatchNorm2d-15         [-1, 32, 112, 112]              64\n",
      "             ReLU-16         [-1, 32, 112, 112]               0\n",
      "        MaxPool2d-17           [-1, 32, 56, 56]               0\n",
      "           Conv2d-18           [-1, 32, 56, 56]             288\n",
      "      BatchNorm2d-19           [-1, 32, 56, 56]              64\n",
      "             ReLU-20           [-1, 32, 56, 56]               0\n",
      "           Conv2d-21           [-1, 64, 56, 56]           2,048\n",
      "      BatchNorm2d-22           [-1, 64, 56, 56]             128\n",
      "             ReLU-23           [-1, 64, 56, 56]               0\n",
      "        MaxPool2d-24           [-1, 64, 28, 28]               0\n",
      "           Conv2d-25           [-1, 64, 28, 28]             576\n",
      "      BatchNorm2d-26           [-1, 64, 28, 28]             128\n",
      "             ReLU-27           [-1, 64, 28, 28]               0\n",
      "           Conv2d-28          [-1, 128, 28, 28]           8,192\n",
      "      BatchNorm2d-29          [-1, 128, 28, 28]             256\n",
      "             ReLU-30          [-1, 128, 28, 28]               0\n",
      "AdaptiveAvgPool2d-31            [-1, 128, 1, 1]               0\n",
      "          Dropout-32                  [-1, 128]               0\n",
      "           Linear-33                   [-1, 11]           1,419\n",
      "================================================================\n",
      "Total params: 14,331\n",
      "Trainable params: 14,331\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.57\n",
      "Forward/backward pass size (MB): 63.55\n",
      "Params size (MB): 0.05\n",
      "Estimated Total Size (MB): 64.18\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# DO NOT modify this block and please make sure that this block can run sucessfully. \n",
    "student_model = get_student_model()\n",
    "summary(student_model, (3, 224, 224), device='cpu')\n",
    "# You have to copy&paste the results of this block to HW13 GradeScope. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-03T03:05:46.591266Z",
     "iopub.status.busy": "2025-07-03T03:05:46.590981Z",
     "iopub.status.idle": "2025-07-03T03:05:48.688857Z",
     "shell.execute_reply": "2025-07-03T03:05:48.687973Z",
     "shell.execute_reply.started": "2025-07-03T03:05:46.591246Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://github.com/pytorch/vision/zipball/v0.10.0\" to /root/.cache/torch/hub/v0.10.0.zip\n",
      "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load provided teacher model (model architecture: resnet18, num_classes=11, test-acc ~= 89.9%)\n",
    "teacher_model = torch.hub.load('pytorch/vision:v0.10.0', 'resnet18', pretrained=False, num_classes=11)\n",
    "# load state dict\n",
    "teacher_ckpt_path = os.path.join(cfg['dataset_root'], \"resnet18_teacher.ckpt\")\n",
    "teacher_model.load_state_dict(torch.load(teacher_ckpt_path, map_location='cpu'))\n",
    "# Now you already know the teacher model's architecture. You can take advantage of it if you want to pass the strong or boss baseline. \n",
    "# Source code of resnet in pytorch: (https://github.com/pytorch/vision/blob/main/torchvision/models/resnet.py)\n",
    "# You can also see the summary of teacher model. There are 11,182,155 parameters totally in the teacher model\n",
    "# summary(teacher_model, (3, 224, 224), device='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-03T04:12:58.312774Z",
     "iopub.status.busy": "2025-07-03T04:12:58.312023Z",
     "iopub.status.idle": "2025-07-03T04:12:58.318207Z",
     "shell.execute_reply": "2025-07-03T04:12:58.317205Z",
     "shell.execute_reply.started": "2025-07-03T04:12:58.312745Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def loss_fn_kd(student_logits, labels, teacher_logits, alpha=0.5, temperature=1.0):\n",
    "    \"\"\"\n",
    "    知识蒸馏损失函数\n",
    "    参数:\n",
    "        student_logits: 学生模型的原始输出logits [batch_size, num_classes]\n",
    "        labels: 真实标签 [batch_size]\n",
    "        teacher_logits: 教师模型的原始输出logits [batch_size, num_classes]\n",
    "        alpha: 蒸馏损失权重 (0-1之间)\n",
    "        temperature: 温度参数(软化概率分布)\n",
    "    返回:\n",
    "        加权后的总损失\n",
    "    \"\"\"\n",
    "    # 1. 计算常规交叉熵损失(学生模型输出与真实标签)\n",
    "    ce_loss = F.cross_entropy(student_logits, labels)\n",
    "    \n",
    "    # 2. 计算KL散度损失(学生与教师模型的软化概率分布)\n",
    "    # 对教师和学生输出应用softmax+log_softmax (带温度参数)\n",
    "    soft_teacher = F.softmax(teacher_logits / temperature, dim=1)\n",
    "    log_soft_student = F.log_softmax(student_logits / temperature, dim=1)\n",
    "    \n",
    "    # 计算KL散度并乘以温度平方(反向传播时梯度缩放补偿)\n",
    "    kl_div_loss = F.kl_div(\n",
    "        log_soft_student, \n",
    "        soft_teacher, \n",
    "        reduction='batchmean'\n",
    "    ) * (temperature ** 2)\n",
    "    \n",
    "    # 3. 加权组合两种损失\n",
    "    total_loss = (1.0 - alpha) * ce_loss + alpha * kl_div_loss\n",
    "    \n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-03T04:13:00.625855Z",
     "iopub.status.busy": "2025-07-03T04:13:00.625395Z",
     "iopub.status.idle": "2025-07-03T04:13:00.631248Z",
     "shell.execute_reply": "2025-07-03T04:13:00.630514Z",
     "shell.execute_reply.started": "2025-07-03T04:13:00.625832Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda\n"
     ]
    }
   ],
   "source": [
    "# choose the loss function by the config\n",
    "if cfg['loss_fn_type'] == 'CE':\n",
    "    # For the classification task, we use cross-entropy as the default loss function.\n",
    "    loss_fn = nn.CrossEntropyLoss() # loss function for simple baseline.\n",
    "\n",
    "if cfg['loss_fn_type'] == 'KD': # KD stands for knowledge distillation\n",
    "    loss_fn = loss_fn_kd # implement loss_fn_kd for the report question and the medium baseline.\n",
    "\n",
    "# You can also adopt other types of knowledge distillation techniques for strong and boss baseline, but use function name other than `loss_fn_kd`\n",
    "# For example:\n",
    "# def loss_fn_custom_kd():\n",
    "#     pass\n",
    "# if cfg['loss_fn_type'] == 'custom_kd':\n",
    "#     loss_fn = loss_fn_custom_kd\n",
    "\n",
    "# \"cuda\" only when GPUs are available.\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "log(f\"device: {device}\")\n",
    "\n",
    "# The number of training epochs and patience.\n",
    "n_epochs = cfg['n_epochs']\n",
    "patience = cfg['patience'] # If no improvement in 'patience' epochs, early stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-03T04:34:55.406499Z",
     "iopub.status.busy": "2025-07-03T04:34:55.405979Z",
     "iopub.status.idle": "2025-07-03T04:34:56.448271Z",
     "shell.execute_reply": "2025-07-03T04:34:56.447485Z",
     "shell.execute_reply.started": "2025-07-03T04:34:55.406476Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2183105bafe46f8a550678f1a28bf1b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/157 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_35/3008579989.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0;31m# Record the loss and accuracy.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0mtrain_batch_len\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimgs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m         \u001b[0mtrain_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mtrain_batch_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m         \u001b[0mtrain_accs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0macc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0mtrain_lens\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_batch_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Initialize a model, and put it on the device specified.\n",
    "student_model.to(device)\n",
    "teacher_model.to(device) # MEDIUM BASELINE\n",
    "\n",
    "# Initialize optimizer, you may fine-tune some hyperparameters such as learning rate on your own.\n",
    "optimizer = torch.optim.Adam(student_model.parameters(), lr=cfg['lr'], weight_decay=cfg['weight_decay']) \n",
    "\n",
    "# Initialize trackers, these are not parameters and should not be changed\n",
    "stale = 0\n",
    "best_acc = 0.0\n",
    "\n",
    "# teacher_model.eval()  # MEDIUM BASELINE\n",
    "for epoch in range(n_epochs):\n",
    "\n",
    "    # ---------- Training ----------\n",
    "    # Make sure the model is in train mode before training.\n",
    "    student_model.train()\n",
    "\n",
    "    # These are used to record information in training.\n",
    "    train_loss = []\n",
    "    train_accs = []\n",
    "    train_lens = []\n",
    "\n",
    "    for batch in tqdm(train_loader):\n",
    "\n",
    "        # A batch consists of image data and corresponding labels.\n",
    "        imgs, labels = batch\n",
    "        imgs = imgs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        #imgs = imgs.half()\n",
    "        #print(imgs.shape,labels.shape)\n",
    "\n",
    "        # Forward the data. (Make sure data and model are on the same device.)\n",
    "        with torch.no_grad():  # MEDIUM BASELINE\n",
    "            teacher_logits = teacher_model(imgs)  # MEDIUM BASELINE\n",
    "        \n",
    "        logits = student_model(imgs)\n",
    "\n",
    "        # Calculate the cross-entropy loss.\n",
    "        # We don't need to apply softmax before computing cross-entropy as it is done automatically.\n",
    "        loss = loss_fn(logits, labels, teacher_logits) # MEDIUM BASELINE\n",
    "        # loss = loss_fn(logits, labels) # SIMPLE BASELINE\n",
    "        # Gradients stored in the parameters in the previous step should be cleared out first.\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Compute the gradients for parameters.\n",
    "        loss.backward()\n",
    "\n",
    "        # Clip the gradient norms for stable training.\n",
    "        grad_norm = nn.utils.clip_grad_norm_(student_model.parameters(), max_norm=cfg['grad_norm_max'])\n",
    "\n",
    "        # Update the parameters with computed gradients.\n",
    "        optimizer.step()\n",
    "\n",
    "        # Compute the accuracy for current batch.\n",
    "        acc = (logits.argmax(dim=-1) == labels).float().sum()\n",
    "\n",
    "        # Record the loss and accuracy.\n",
    "        train_batch_len = len(imgs)\n",
    "        train_loss.append(loss.item() * train_batch_len)\n",
    "        train_accs.append(acc)\n",
    "        train_lens.append(train_batch_len)\n",
    "        \n",
    "    train_loss = sum(train_loss) / sum(train_lens)\n",
    "    train_acc = sum(train_accs) / sum(train_lens)\n",
    "\n",
    "    # Print the information.\n",
    "    log(f\"[ Train | {epoch + 1:03d}/{n_epochs:03d} ] loss = {train_loss:.5f}, acc = {train_acc:.5f}\")\n",
    "\n",
    "    # ---------- Validation ----------\n",
    "    # Make sure the model is in eval mode so that some modules like dropout are disabled and work normally.\n",
    "    student_model.eval()\n",
    "\n",
    "    # These are used to record information in validation.\n",
    "    valid_loss = []\n",
    "    valid_accs = []\n",
    "    valid_lens = []\n",
    "\n",
    "    # Iterate the validation set by batches.\n",
    "    for batch in tqdm(valid_loader):\n",
    "\n",
    "        # A batch consists of image data and corresponding labels.\n",
    "        imgs, labels = batch\n",
    "        imgs = imgs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # We don't need gradient in validation.\n",
    "        # Using torch.no_grad() accelerates the forward process.\n",
    "        with torch.no_grad():\n",
    "            logits = student_model(imgs)\n",
    "            teacher_logits = teacher_model(imgs) # MEDIUM BASELINE\n",
    "\n",
    "        # We can still compute the loss (but not the gradient).\n",
    "        loss = loss_fn(logits, labels, teacher_logits) # MEDIUM BASELINE\n",
    "        # loss = loss_fn(logits, labels) # SIMPLE BASELINE\n",
    "\n",
    "        # Compute the accuracy for current batch.\n",
    "        acc = (logits.argmax(dim=-1) == labels).float().sum()\n",
    "\n",
    "        # Record the loss and accuracy.\n",
    "        batch_len = len(imgs)\n",
    "        valid_loss.append(loss.item() * batch_len)\n",
    "        valid_accs.append(acc)\n",
    "        valid_lens.append(batch_len)\n",
    "        #break\n",
    "\n",
    "    # The average loss and accuracy for entire validation set is the average of the recorded values.\n",
    "    valid_loss = sum(valid_loss) / sum(valid_lens)\n",
    "    valid_acc = sum(valid_accs) / sum(valid_lens)\n",
    "\n",
    "    # update logs\n",
    "    \n",
    "    if valid_acc > best_acc:\n",
    "        log(f\"[ Valid | {epoch + 1:03d}/{n_epochs:03d} ] loss = {valid_loss:.5f}, acc = {valid_acc:.5f} -> best\")\n",
    "    else:\n",
    "        log(f\"[ Valid | {epoch + 1:03d}/{n_epochs:03d} ] loss = {valid_loss:.5f}, acc = {valid_acc:.5f}\")\n",
    "\n",
    "\n",
    "    # save models\n",
    "    if valid_acc > best_acc:\n",
    "        log(f\"Best model found at epoch {epoch+1}, saving model\")\n",
    "        torch.save(student_model.state_dict(), f\"{save_path}/student_best.ckpt\") # only save best to prevent output memory exceed error\n",
    "        best_acc = valid_acc\n",
    "        stale = 0\n",
    "    else:\n",
    "        stale += 1\n",
    "        if stale > patience:\n",
    "            log(f\"No improvment {patience} consecutive epochs, early stopping\")\n",
    "            break\n",
    "log(\"Finish training\")\n",
    "log_fw.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-03T04:33:42.406285Z",
     "iopub.status.busy": "2025-07-03T04:33:42.406014Z",
     "iopub.status.idle": "2025-07-03T04:33:42.414833Z",
     "shell.execute_reply": "2025-07-03T04:33:42.414188Z",
     "shell.execute_reply.started": "2025-07-03T04:33:42.406264Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One /kaggle/input/homework13-dataset/Food-11/evaluation sample /kaggle/input/homework13-dataset/Food-11/evaluation/0000.jpg\n"
     ]
    }
   ],
   "source": [
    "# create dataloader for evaluation\n",
    "eval_set = FoodDataset(os.path.join(cfg['dataset_root'], \"evaluation\"), tfm=test_tfm)\n",
    "eval_loader = DataLoader(eval_set, batch_size=cfg['batch_size'], shuffle=False, num_workers=0, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-03T04:33:54.664407Z",
     "iopub.status.busy": "2025-07-03T04:33:54.663973Z",
     "iopub.status.idle": "2025-07-03T04:34:22.115368Z",
     "shell.execute_reply": "2025-07-03T04:34:22.114850Z",
     "shell.execute_reply.started": "2025-07-03T04:33:54.664382Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a591e23057a54639a825a8ac0de7f639",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/35 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load model from {exp_name}/student_best.ckpt\n",
    "student_model_best = get_student_model() # get a new student model to avoid reference before assignment.\n",
    "ckpt_path = f\"{save_path}/student_best.ckpt\" # the ckpt path of the best student model.\n",
    "student_model_best.load_state_dict(torch.load(ckpt_path, map_location='cpu')) # load the state dict and set it to the student model\n",
    "student_model_best.to(device) # set the student model to device\n",
    "\n",
    "# Start evaluate\n",
    "student_model_best.eval()\n",
    "eval_preds = [] # storing predictions of the evaluation dataset\n",
    "\n",
    "# Iterate the validation set by batches.\n",
    "for batch in tqdm(eval_loader):\n",
    "    # A batch consists of image data and corresponding labels.\n",
    "    imgs, _ = batch\n",
    "    # We don't need gradient in evaluation.\n",
    "    # Using torch.no_grad() accelerates the forward process.\n",
    "    with torch.no_grad():\n",
    "        logits = student_model_best(imgs.to(device))\n",
    "        preds = list(logits.argmax(dim=-1).squeeze().cpu().numpy())\n",
    "    # loss and acc can not be calculated because we do not have the true labels of the evaluation set.\n",
    "    eval_preds += preds\n",
    "\n",
    "def pad4(i):\n",
    "    return \"0\"*(4-len(str(i))) + str(i)\n",
    "\n",
    "# Save prediction results\n",
    "ids = [pad4(i) for i in range(0,len(eval_set))]\n",
    "categories = eval_preds\n",
    "\n",
    "df = pd.DataFrame()\n",
    "df['Id'] = ids\n",
    "df['Category'] = categories\n",
    "df.to_csv(f\"{save_path}/submission.csv\", index=False) # now you can download the submission.csv and upload it to the kaggle competition."
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 7790440,
     "sourceId": 12356844,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31041,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
