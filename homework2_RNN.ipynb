{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-24T02:19:40.258806Z",
     "iopub.status.busy": "2025-06-24T02:19:40.258165Z",
     "iopub.status.idle": "2025-06-24T02:19:40.381845Z",
     "shell.execute_reply": "2025-06-24T02:19:40.381168Z",
     "shell.execute_reply.started": "2025-06-24T02:19:40.258779Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!ls -F /kaggle/input/homework2dataset/libriphone/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-24T02:19:42.166329Z",
     "iopub.status.busy": "2025-06-24T02:19:42.165594Z",
     "iopub.status.idle": "2025-06-24T02:19:50.181236Z",
     "shell.execute_reply": "2025-06-24T02:19:50.180634Z",
     "shell.execute_reply.started": "2025-06-24T02:19:42.166281Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import random\n",
    "import os\n",
    "import gc\n",
    "\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "\n",
    "def same_seeds(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    # cudnn.benchmark = False 可能会略微降低性能，但确保每次运行结果一致\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "def load_feat(path):\n",
    "\n",
    "    feat = torch.load(path)\n",
    "    return feat\n",
    "\n",
    "def shift(x, n):\n",
    "\n",
    "    if n < 0:\n",
    "        left = x[0].repeat(-n, 1)\n",
    "        right = x[:n]\n",
    "    elif n > 0:\n",
    "        right = x[-1].repeat(n, 1)\n",
    "        left = x[n:]\n",
    "    else:\n",
    "        return x\n",
    "    return torch.cat((left, right), dim=0)\n",
    "\n",
    "\n",
    "def concat_feat(x, concat_n):\n",
    "\n",
    "    assert concat_n % 2 == 1\n",
    "    if concat_n < 2:\n",
    "        return x\n",
    "\n",
    "    seq_len, feature_dim = x.size(0), x.size(1)\n",
    "\n",
    "    x_padded = x.repeat(1, concat_n).view(seq_len, concat_n, feature_dim).permute(1, 0, 2)\n",
    "\n",
    "    mid = (concat_n // 2)\n",
    "    for r_idx in range(1, mid + 1):\n",
    "        x_padded[mid + r_idx, :, :] = shift(x_padded[mid + r_idx, :, :].clone(), r_idx)\n",
    "        x_padded[mid - r_idx, :, :] = shift(x_padded[mid - r_idx, :, :].clone(), -r_idx)\n",
    "\n",
    "    return x_padded.permute(1, 0, 2).reshape(seq_len, concat_n * feature_dim)\n",
    "\n",
    "def preprocess_data(split, feat_dir, phone_path, concat_nframes, train_ratio=0.8):\n",
    "\n",
    "    class_num = 41\n",
    "\n",
    "    if split == 'train' or split == 'val':\n",
    "        mode = 'train'\n",
    "    elif split == 'test':\n",
    "        mode = 'test'\n",
    "    else:\n",
    "        raise ValueError('Invalid \\'split\\' argument for dataset: PhoneDataset!')\n",
    "\n",
    "    label_dict = {}\n",
    "    if mode == 'train':\n",
    "        with open(os.path.join(phone_path, f'{mode}_labels.txt')) as f:\n",
    "            for line in f.readlines():\n",
    "                line = line.strip('\\n').split(' ')\n",
    "                label_dict[line[0]] = [int(p) for p in line[1:]]\n",
    "\n",
    "        with open(os.path.join(phone_path, 'train_split.txt')) as f:\n",
    "            usage_list = f.readlines()\n",
    "        random.shuffle(usage_list)\n",
    "        train_len = int(len(usage_list) * train_ratio)\n",
    "        usage_list = [line.strip('\\n') for line in usage_list]\n",
    "        usage_list = usage_list[:train_len] if split == 'train' else usage_list[train_len:]\n",
    "\n",
    "    elif mode == 'test':\n",
    "        with open(os.path.join(phone_path, 'test_split.txt')) as f:\n",
    "            usage_list = f.readlines()\n",
    "        usage_list = [line.strip('\\n') for line in usage_list]\n",
    "\n",
    "    print(f'[Dataset] - # phone classes: {class_num}, number of utterances for {split}: {len(usage_list)}')\n",
    "\n",
    "    all_feats = []\n",
    "    all_labels = [] if mode == 'train' else None\n",
    "\n",
    "    for fname in tqdm(usage_list, desc=f\"Loading {split} data\"):\n",
    "        feat = load_feat(os.path.join(feat_dir, mode, f'{fname}.pt'))\n",
    "        feat = concat_feat(feat, concat_nframes)  # 应用局部上下文拼接\n",
    "\n",
    "        all_feats.append(feat)\n",
    "\n",
    "        if mode == 'train':\n",
    "            label = torch.LongTensor(label_dict[fname])\n",
    "            all_labels.append(label)\n",
    "\n",
    "    print(f'[INFO] {split} set loaded. Total {len(all_feats)} utterances.')\n",
    "\n",
    "    if mode == 'train':\n",
    "        return all_feats, all_labels\n",
    "    else:\n",
    "        return all_feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-24T02:23:36.581581Z",
     "iopub.status.busy": "2025-06-24T02:23:36.581037Z",
     "iopub.status.idle": "2025-06-24T02:23:45.979471Z",
     "shell.execute_reply": "2025-06-24T02:23:45.978716Z",
     "shell.execute_reply.started": "2025-06-24T02:23:36.581558Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class LibriDataset(Dataset):\n",
    "\n",
    "    def __init__(self, X, y=None):\n",
    "        self.data = X\n",
    "        self.label = y\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if self.label is not None:\n",
    "            return self.data[idx], self.label[idx]\n",
    "        else:\n",
    "            return self.data[idx]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "\n",
    "class PadSequence:\n",
    "\n",
    "    def __call__(self, batch):\n",
    "\n",
    "        has_labels = len(batch[0]) == 2\n",
    "\n",
    "        if has_labels:\n",
    "            features = [item[0] for item in batch]\n",
    "            labels = [item[1] for item in batch]\n",
    "        else:\n",
    "            features = [item for item in batch]\n",
    "\n",
    "        max_len = max([f.size(0) for f in features])\n",
    "\n",
    "        padded_features = []\n",
    "        for f in features:\n",
    "            pad_tensor = torch.zeros(max_len - f.size(0), f.size(1), dtype=f.dtype)\n",
    "            padded_features.append(torch.cat([f, pad_tensor], dim=0))\n",
    "        padded_features = torch.stack(padded_features)  # (batch_size, max_len, feature_dim)\n",
    "\n",
    "        if has_labels:\n",
    "            # 填充标签序列 (-100 是一个常见的忽略索引，或者可以用0)\n",
    "            padded_labels = []\n",
    "            for l in labels:\n",
    "                pad_tensor = torch.full((max_len - l.size(0),), -100, dtype=l.dtype)\n",
    "                padded_labels.append(torch.cat([l, pad_tensor], dim=0))\n",
    "            padded_labels = torch.stack(padded_labels)\n",
    "            return padded_features, padded_labels\n",
    "        else:\n",
    "            return padded_features\n",
    "\n",
    "# --- LSTM 分类器模型 ---\n",
    "class LSTMClassifier(nn.Module):\n",
    "\n",
    "    def __init__(self, input_feature_dim, hidden_dim, num_layers, output_dim, dropout_p=0.5, bidirectional=True):\n",
    "        super(LSTMClassifier, self).__init__()\n",
    "\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.bidirectional = bidirectional\n",
    "\n",
    "        self.lstm = nn.LSTM(input_feature_dim, hidden_dim, num_layers,\n",
    "                            batch_first=True,  # 输入形状是 (batch, seq, feature)\n",
    "                            dropout=dropout_p if num_layers > 1 else 0,  # 多层时才应用dropout\n",
    "                            bidirectional=bidirectional)\n",
    "\n",
    "        fc_input_dim = hidden_dim * 2 if bidirectional else hidden_dim\n",
    "        self.fc = nn.Linear(fc_input_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        num_directions = 2 if self.bidirectional else 1\n",
    "\n",
    "        h0 = torch.zeros(self.num_layers * num_directions, x.size(0), self.hidden_dim).to(x.device)\n",
    "        c0 = torch.zeros(self.num_layers * num_directions, x.size(0), self.hidden_dim).to(x.device)\n",
    "\n",
    "        output, (hn, cn) = self.lstm(x, (h0.detach(), c0.detach()))\n",
    "        output = self.fc(output.reshape(-1, output.size(2)))\n",
    "        return output\n",
    "\n",
    "concat_nframes = 5\n",
    "train_ratio = 0.75\n",
    "\n",
    "seed = 42\n",
    "batch_size = 32\n",
    "num_epoch = 30\n",
    "learning_rate = 1e-3\n",
    "model_path = '/kaggle/working/lstm_phone_classifier.ckpt'\n",
    "\n",
    "\n",
    "original_feature_dim = 39\n",
    "input_feature_dim_for_lstm = original_feature_dim * concat_nframes\n",
    "lstm_hidden_dim = 256\n",
    "lstm_num_layers = 3\n",
    "lstm_dropout_p = 0.3\n",
    "bidirectional = True\n",
    "\n",
    "class_num = 41  # 电话（音素）分类的类别数量\n",
    "\n",
    "same_seeds(seed)\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f'DEVICE: {device}')\n",
    "print(\"--- Preparing Data ---\")\n",
    "train_X, train_y = preprocess_data(split='train', feat_dir='/kaggle/input/homework2dataset/libriphone/feat', phone_path='/kaggle/input/homework2dataset/libriphone', concat_nframes=concat_nframes,\n",
    "                                   train_ratio=train_ratio)\n",
    "val_X, val_y = preprocess_data(split='val', feat_dir='/kaggle/input/homework2dataset/libriphone/feat', phone_path='/kaggle/input/homework2dataset/libriphone', concat_nframes=concat_nframes,\n",
    "                               train_ratio=train_ratio)\n",
    "\n",
    "train_set = LibriDataset(train_X, train_y)\n",
    "val_set = LibriDataset(val_X, val_y)\n",
    "\n",
    "\n",
    "del train_X, train_y, val_X, val_y\n",
    "gc.collect()\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True, collate_fn=PadSequence())\n",
    "val_loader = DataLoader(val_set, batch_size=batch_size, shuffle=False, collate_fn=PadSequence())\n",
    "print(\"--- Data Prepared ---\")\n",
    "\n",
    "model = LSTMClassifier(\n",
    "    input_feature_dim=input_feature_dim_for_lstm,\n",
    "    hidden_dim=lstm_hidden_dim,\n",
    "    num_layers=lstm_num_layers,\n",
    "    output_dim=class_num,\n",
    "    dropout_p=lstm_dropout_p,\n",
    "    bidirectional=bidirectional\n",
    ").to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=-100)\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=5, verbose=True)\n",
    "\n",
    "print(f\"--- Model Initialized: {model.__class__.__name__} ---\")\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-24T02:23:48.707257Z",
     "iopub.status.busy": "2025-06-24T02:23:48.706988Z",
     "iopub.status.idle": "2025-06-24T02:39:11.608485Z",
     "shell.execute_reply": "2025-06-24T02:39:11.607648Z",
     "shell.execute_reply.started": "2025-06-24T02:23:48.707238Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(\"--- Start Training ---\")\n",
    "best_val_acc = 0.0\n",
    "\n",
    "for epoch in range(num_epoch):\n",
    "    model.train()\n",
    "\n",
    "    total_train_loss = 0.0\n",
    "    correct_train_predictions = 0\n",
    "    total_train_effective_labels = 0\n",
    "\n",
    "\n",
    "    for batch_idx, (features, labels) in enumerate(tqdm(train_loader, desc=f\"Epoch {epoch + 1}/{num_epoch} [Train]\")):\n",
    "        features = features.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(features)\n",
    "\n",
    "        active_labels = labels.view(-1)\n",
    "        active_outputs = outputs[active_labels != -100]\n",
    "        active_labels = active_labels[active_labels != -100]\n",
    "\n",
    "        if active_labels.numel() == 0:\n",
    "            continue\n",
    "\n",
    "        loss = criterion(active_outputs, active_labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        _, train_pred = torch.max(active_outputs, 1)\n",
    "        correct_train_predictions += (train_pred == active_labels).sum().item()\n",
    "        total_train_loss += loss.item() * len(active_labels)\n",
    "        total_train_effective_labels += len(active_labels)\n",
    "\n",
    "    model.eval()\n",
    "    total_val_loss = 0.0\n",
    "    correct_val_predictions = 0\n",
    "    total_val_effective_labels = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (features, labels) in enumerate(tqdm(val_loader, desc=f\"Epoch {epoch + 1}/{num_epoch} [Val]\")):\n",
    "            features = features.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(features)\n",
    "\n",
    "            active_labels = labels.view(-1)\n",
    "            active_outputs = outputs[active_labels != -100]\n",
    "            active_labels = active_labels[active_labels != -100]\n",
    "\n",
    "            if active_labels.numel() == 0:\n",
    "                continue\n",
    "\n",
    "            loss = criterion(active_outputs, active_labels)\n",
    "\n",
    "            _, val_pred = torch.max(active_outputs, 1)\n",
    "            correct_val_predictions += (val_pred == active_labels).sum().item()\n",
    "            total_val_loss += loss.item() * len(active_labels)\n",
    "            total_val_effective_labels += len(active_labels)\n",
    "\n",
    "\n",
    "    avg_train_acc = correct_train_predictions / total_train_effective_labels if total_train_effective_labels > 0 else 0\n",
    "    avg_train_loss = total_train_loss / total_train_effective_labels if total_train_effective_labels > 0 else 0\n",
    "    avg_val_acc = correct_val_predictions / total_val_effective_labels if total_val_effective_labels > 0 else 0\n",
    "    avg_val_loss = total_val_loss / total_val_effective_labels if total_val_effective_labels > 0 else 0\n",
    "\n",
    "\n",
    "    print(f'[{epoch + 1:03d}/{num_epoch:03d}] '\n",
    "          f'Train Acc: {avg_train_acc:.5f} Loss: {avg_train_loss:.5f} | '\n",
    "          f'Val Acc: {avg_val_acc:.5f} Loss: {avg_val_loss:.5f}')\n",
    "\n",
    "\n",
    "    scheduler.step(avg_val_acc)\n",
    "\n",
    "    if avg_val_acc > best_val_acc:\n",
    "        best_val_acc = avg_val_acc\n",
    "        torch.save(model.state_dict(), model_path)\n",
    "        print(f'Saving model with Val Acc: {best_val_acc:.5f}')\n",
    "\n",
    "del train_set, val_set, train_loader, val_loader\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "print(\"--- Training Finished ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-06-24T02:40:09.189336Z",
     "iopub.status.busy": "2025-06-24T02:40:09.189065Z",
     "iopub.status.idle": "2025-06-24T02:40:24.440238Z",
     "shell.execute_reply": "2025-06-24T02:40:24.439618Z",
     "shell.execute_reply.started": "2025-06-24T02:40:09.189318Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(\"--- Start Testing ---\")\n",
    "test_X = preprocess_data(split='test', feat_dir='/kaggle/input/homework2dataset/libriphone/feat', phone_path='/kaggle/input/homework2dataset/libriphone', concat_nframes=concat_nframes)\n",
    "test_set = LibriDataset(test_X, None)\n",
    "test_loader = DataLoader(test_set, batch_size=batch_size, shuffle=False, collate_fn=PadSequence())\n",
    "\n",
    "model = LSTMClassifier(\n",
    "    input_feature_dim=input_feature_dim_for_lstm,\n",
    "    hidden_dim=lstm_hidden_dim,\n",
    "    num_layers=lstm_num_layers,\n",
    "    output_dim=class_num,\n",
    "    dropout_p=lstm_dropout_p,\n",
    "    bidirectional=bidirectional\n",
    ").to(device)\n",
    "model.load_state_dict(torch.load(model_path))\n",
    "\n",
    "\n",
    "model.eval()\n",
    "all_predictions = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_idx, features in enumerate(tqdm(test_loader, desc=\"[Test Prediction]\")):\n",
    "        features = features.to(device)\n",
    "        outputs = model(features)\n",
    "\n",
    "        _, test_pred = torch.max(outputs, 1)\n",
    "        all_predictions.extend(test_pred.cpu().numpy())\n",
    "\n",
    "with open('/kaggle/working/prediction.csv', 'w') as f:\n",
    "    f.write('Id,Class\\n')\n",
    "    for i, y in enumerate(all_predictions):\n",
    "        f.write(f'{i},{y}\\n')\n",
    "\n",
    "print(\"Prediction file 'prediction.csv' generated.\")\n",
    "print(\"--- Testing Finished ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "print(os.listdir(\"/kaggle/working/.virtual_documents\"))"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 7724463,
     "sourceId": 12258362,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31041,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
