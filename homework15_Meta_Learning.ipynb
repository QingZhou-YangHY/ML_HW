{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Meta Learning : Learn how to learn\n",
    "Machine Learning = Looking for a function\n",
    "Step 1: Function with unknown\n",
    "Step 2: Define loss function\n",
    "Step 3: Optimization\n",
    "\n",
    "Learning algorithm:Hand-crafted\n",
    "机器自己学出来的就是learnable components\n",
    "\n",
    "Meta Learning是跨任务学习Across-task Training,Machine Learning是Within-task Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2025-07-03T07:52:26.234950Z",
     "iopub.status.busy": "2025-07-03T07:52:26.234788Z",
     "iopub.status.idle": "2025-07-03T07:52:31.523115Z",
     "shell.execute_reply": "2025-07-03T07:52:31.522306Z",
     "shell.execute_reply.started": "2025-07-03T07:52:26.234934Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2025-07-03 07:52:26--  https://www.dropbox.com/s/pqeym3n4jly5e89/Omniglot.tar.gz?dl=1\n",
      "Resolving www.dropbox.com (www.dropbox.com)... 162.125.3.18, 2620:100:6018:18::a27d:312\n",
      "Connecting to www.dropbox.com (www.dropbox.com)|162.125.3.18|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://www.dropbox.com/scl/fi/peae3bis6c9i96zsmmbzc/Omniglot.tar.gz?rlkey=v9ljhktg1wiy3x9otdz3p7k8c&dl=1 [following]\n",
      "--2025-07-03 07:52:26--  https://www.dropbox.com/scl/fi/peae3bis6c9i96zsmmbzc/Omniglot.tar.gz?rlkey=v9ljhktg1wiy3x9otdz3p7k8c&dl=1\n",
      "Reusing existing connection to www.dropbox.com:443.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://ucc8b4901ceb6b316a8bbf2bb94c.dl.dropboxusercontent.com/cd/0/inline/Csx3YKOvieFHZWQeYexq-yQSQXgGPQ8RkBrJn9CRnG0U-lDcuiFOYMt-ZQTbN1RP6l8xzEOD9VQ54O-BaWPISvo-IQAYFXmOO-F6C3RzYMb4N81nMl4yDaIn-HMNfF0H0J-_4ZHtqMMLIRxxDVwrfIlt/file?dl=1# [following]\n",
      "--2025-07-03 07:52:26--  https://ucc8b4901ceb6b316a8bbf2bb94c.dl.dropboxusercontent.com/cd/0/inline/Csx3YKOvieFHZWQeYexq-yQSQXgGPQ8RkBrJn9CRnG0U-lDcuiFOYMt-ZQTbN1RP6l8xzEOD9VQ54O-BaWPISvo-IQAYFXmOO-F6C3RzYMb4N81nMl4yDaIn-HMNfF0H0J-_4ZHtqMMLIRxxDVwrfIlt/file?dl=1\n",
      "Resolving ucc8b4901ceb6b316a8bbf2bb94c.dl.dropboxusercontent.com (ucc8b4901ceb6b316a8bbf2bb94c.dl.dropboxusercontent.com)... 162.125.3.15, 2620:100:6018:15::a27d:30f\n",
      "Connecting to ucc8b4901ceb6b316a8bbf2bb94c.dl.dropboxusercontent.com (ucc8b4901ceb6b316a8bbf2bb94c.dl.dropboxusercontent.com)|162.125.3.15|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: /cd/0/inline2/Cswr8lp_snJoMBWD_zRJCWWvHk2ZksS-D_DkbsGwmgHnMZqB7VAYHX2_5mS6Q_hshhKq3ovhEc4EkpplknjXR-GnVpgr2JTlSiFw46uesK1KBr7oLSA_ePP9pMCHyH03CJSteOvMd3IavB0Fgm6CePPHIdpB9ZozDI6ztm3PLhfQpmLkBkb4_CGcQCC0ZvKlxYp5qtgHaLacApL-GYNP4ELXHybHWgu4cfndqhVI22s0o7wIx8RLeAY9jM4VROpmd48t44TsgDnPGlRczJvdWnWsAoZyxFQSXRKZ10sdv-2E6SLApObYewhvsx6hxZQADsoYW-XK40V2mvLKlGVKkdmldn4-34CwTzSPI6-ZH9a3CrVWd8e5Kne7VtbNN49iEaw/file?dl=1 [following]\n",
      "--2025-07-03 07:52:27--  https://ucc8b4901ceb6b316a8bbf2bb94c.dl.dropboxusercontent.com/cd/0/inline2/Cswr8lp_snJoMBWD_zRJCWWvHk2ZksS-D_DkbsGwmgHnMZqB7VAYHX2_5mS6Q_hshhKq3ovhEc4EkpplknjXR-GnVpgr2JTlSiFw46uesK1KBr7oLSA_ePP9pMCHyH03CJSteOvMd3IavB0Fgm6CePPHIdpB9ZozDI6ztm3PLhfQpmLkBkb4_CGcQCC0ZvKlxYp5qtgHaLacApL-GYNP4ELXHybHWgu4cfndqhVI22s0o7wIx8RLeAY9jM4VROpmd48t44TsgDnPGlRczJvdWnWsAoZyxFQSXRKZ10sdv-2E6SLApObYewhvsx6hxZQADsoYW-XK40V2mvLKlGVKkdmldn4-34CwTzSPI6-ZH9a3CrVWd8e5Kne7VtbNN49iEaw/file?dl=1\n",
      "Reusing existing connection to ucc8b4901ceb6b316a8bbf2bb94c.dl.dropboxusercontent.com:443.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 5718170 (5.5M) [application/binary]\n",
      "Saving to: ‘/kaggle/working//Omniglot.tar.gz’\n",
      "\n",
      "/kaggle/working//Om 100%[===================>]   5.45M  --.-KB/s    in 0.09s   \n",
      "\n",
      "2025-07-03 07:52:27 (62.5 MB/s) - ‘/kaggle/working//Omniglot.tar.gz’ saved [5718170/5718170]\n",
      "\n",
      "--2025-07-03 07:52:27--  https://www.dropbox.com/s/nlvokertmksfc42/Omniglot-test.tar.gz?dl=1\n",
      "Resolving www.dropbox.com (www.dropbox.com)... 162.125.3.18, 2620:100:6018:18::a27d:312\n",
      "Connecting to www.dropbox.com (www.dropbox.com)|162.125.3.18|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://www.dropbox.com/scl/fi/7y3ivzl5b8oq5ogj7pfq9/Omniglot-test.tar.gz?rlkey=cm9x0et2xwfl9trijmwkh0qz0&dl=1 [following]\n",
      "--2025-07-03 07:52:28--  https://www.dropbox.com/scl/fi/7y3ivzl5b8oq5ogj7pfq9/Omniglot-test.tar.gz?rlkey=cm9x0et2xwfl9trijmwkh0qz0&dl=1\n",
      "Reusing existing connection to www.dropbox.com:443.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://ucb168d8aa96a02d858064147aec.dl.dropboxusercontent.com/cd/0/inline/CsxJ68gz27LIHip1gt84obfQU6Z5P-9GQzHWVd_uXa7_-uyR4dahbzSpK793a-S0nQtQsDk6FDccutKvgf9kzB0IZ2wBuqb-mELIzfE0b_jkybbfxWvtF_HwFmmQlW8MH0Nwq3AswK4w_jmuvQKExu1P/file?dl=1# [following]\n",
      "--2025-07-03 07:52:28--  https://ucb168d8aa96a02d858064147aec.dl.dropboxusercontent.com/cd/0/inline/CsxJ68gz27LIHip1gt84obfQU6Z5P-9GQzHWVd_uXa7_-uyR4dahbzSpK793a-S0nQtQsDk6FDccutKvgf9kzB0IZ2wBuqb-mELIzfE0b_jkybbfxWvtF_HwFmmQlW8MH0Nwq3AswK4w_jmuvQKExu1P/file?dl=1\n",
      "Resolving ucb168d8aa96a02d858064147aec.dl.dropboxusercontent.com (ucb168d8aa96a02d858064147aec.dl.dropboxusercontent.com)... 162.125.3.15, 2620:100:6018:15::a27d:30f\n",
      "Connecting to ucb168d8aa96a02d858064147aec.dl.dropboxusercontent.com (ucb168d8aa96a02d858064147aec.dl.dropboxusercontent.com)|162.125.3.15|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: /cd/0/inline2/Csyb5PPjdj2KH6EeO4ya9vxFxTFcW8PuUdfAQOzh1RE4vzkF1XgAro5CqOpjBTvoZly2KHd3C_Zo3BOZvhS05X61q2tvRU_pTmS51DTD_mtHLN1Z-73EQkx38hTkGA2JygeZqf9huoIQFe0oK8vVgBna1FaLuzqUJrpMNtIfzKmhhvrHZmCFUFIWhhuXu1oYj-W0RKTtc6coOe00DdgYU7AEWKRJOKXVLZbGoMzJAL87Iq71LkCEj8m1r7R-RFh52o2Xkv6j4AnvNXal_Sb9dUaExOomjtTCeKQNVT8s4fh6LmAzmZYLZ80aZyJw1LxhqVu6-9B-n_J0x9tnfjWMtshPUGKajjANRaooKKOux1AaPoa9_UwVP43BRm-B_ROLWWU/file?dl=1 [following]\n",
      "--2025-07-03 07:52:29--  https://ucb168d8aa96a02d858064147aec.dl.dropboxusercontent.com/cd/0/inline2/Csyb5PPjdj2KH6EeO4ya9vxFxTFcW8PuUdfAQOzh1RE4vzkF1XgAro5CqOpjBTvoZly2KHd3C_Zo3BOZvhS05X61q2tvRU_pTmS51DTD_mtHLN1Z-73EQkx38hTkGA2JygeZqf9huoIQFe0oK8vVgBna1FaLuzqUJrpMNtIfzKmhhvrHZmCFUFIWhhuXu1oYj-W0RKTtc6coOe00DdgYU7AEWKRJOKXVLZbGoMzJAL87Iq71LkCEj8m1r7R-RFh52o2Xkv6j4AnvNXal_Sb9dUaExOomjtTCeKQNVT8s4fh6LmAzmZYLZ80aZyJw1LxhqVu6-9B-n_J0x9tnfjWMtshPUGKajjANRaooKKOux1AaPoa9_UwVP43BRm-B_ROLWWU/file?dl=1\n",
      "Reusing existing connection to ucb168d8aa96a02d858064147aec.dl.dropboxusercontent.com:443.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1822122 (1.7M) [application/binary]\n",
      "Saving to: ‘/kaggle/working//Omniglot-test.tar.gz’\n",
      "\n",
      "/kaggle/working//Om 100%[===================>]   1.74M  --.-KB/s    in 0.07s   \n",
      "\n",
      "2025-07-03 07:52:29 (25.8 MB/s) - ‘/kaggle/working//Omniglot-test.tar.gz’ saved [1822122/1822122]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "workspace_dir = '/kaggle/working/'\n",
    "\n",
    "# Download dataset\n",
    "!wget https://www.dropbox.com/s/pqeym3n4jly5e89/Omniglot.tar.gz?dl=1 \\\n",
    "    -O \"{workspace_dir}/Omniglot.tar.gz\"\n",
    "!wget https://www.dropbox.com/s/nlvokertmksfc42/Omniglot-test.tar.gz?dl=1 \\\n",
    "    -O \"{workspace_dir}/Omniglot-test.tar.gz\"\n",
    "\n",
    "# Use `tar' command to decompress\n",
    "!tar -zxf \"{workspace_dir}/Omniglot.tar.gz\" -C \"{workspace_dir}/\"\n",
    "!tar -zxf \"{workspace_dir}/Omniglot-test.tar.gz\" -C \"{workspace_dir}/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-03T07:52:55.883131Z",
     "iopub.status.busy": "2025-07-03T07:52:55.882552Z",
     "iopub.status.idle": "2025-07-03T07:53:02.439080Z",
     "shell.execute_reply": "2025-07-03T07:53:02.438488Z",
     "shell.execute_reply.started": "2025-07-03T07:52:55.883086Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEVICE = cuda\n"
     ]
    }
   ],
   "source": [
    "# Import modules we need\n",
    "import glob, random\n",
    "from collections import OrderedDict\n",
    "\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import torch, torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from PIL import Image\n",
    "from IPython.display import display\n",
    "\n",
    "# Check device\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"DEVICE = {device}\")\n",
    "\n",
    "# Fix random seeds\n",
    "random_seed = 0\n",
    "random.seed(random_seed)\n",
    "np.random.seed(random_seed)\n",
    "torch.manual_seed(random_seed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-03T07:53:06.703999Z",
     "iopub.status.busy": "2025-07-03T07:53:06.703730Z",
     "iopub.status.idle": "2025-07-03T07:53:06.709160Z",
     "shell.execute_reply": "2025-07-03T07:53:06.708602Z",
     "shell.execute_reply.started": "2025-07-03T07:53:06.703977Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def ConvBlock(in_ch: int, out_ch: int):\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(in_ch, out_ch, 3, padding=1),\n",
    "        nn.BatchNorm2d(out_ch),\n",
    "        nn.ReLU(),\n",
    "        nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "    )\n",
    "\n",
    "\n",
    "def ConvBlockFunction(x, w, b, w_bn, b_bn):\n",
    "    x = F.conv2d(x, w, b, padding=1)\n",
    "    x = F.batch_norm(\n",
    "        x, running_mean=None, running_var=None, weight=w_bn, bias=b_bn, training=True\n",
    "    )\n",
    "    x = F.relu(x)\n",
    "    x = F.max_pool2d(x, kernel_size=2, stride=2)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-03T07:53:11.825035Z",
     "iopub.status.busy": "2025-07-03T07:53:11.824279Z",
     "iopub.status.idle": "2025-07-03T07:53:11.831722Z",
     "shell.execute_reply": "2025-07-03T07:53:11.830948Z",
     "shell.execute_reply.started": "2025-07-03T07:53:11.825003Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class Classifier(nn.Module):\n",
    "    def __init__(self, in_ch, k_way):\n",
    "        super(Classifier, self).__init__()\n",
    "        self.conv1 = ConvBlock(in_ch, 64)\n",
    "        self.conv2 = ConvBlock(64, 64)\n",
    "        self.conv3 = ConvBlock(64, 64)\n",
    "        self.conv4 = ConvBlock(64, 64)\n",
    "        self.logits = nn.Linear(64, k_way)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.conv4(x)\n",
    "        x = x.view(x.shape[0], -1)\n",
    "        x = self.logits(x)\n",
    "        return x\n",
    "\n",
    "    def functional_forward(self, x, params):\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "        x: input images [batch, 1, 28, 28]\n",
    "        params: model parameters,\n",
    "                i.e. weights and biases of convolution\n",
    "                     and weights and biases of\n",
    "                                   batch normalization\n",
    "                type is an OrderedDict\n",
    "\n",
    "        Arguments:\n",
    "        x: input images [batch, 1, 28, 28]\n",
    "        params: The model parameters,\n",
    "                i.e. weights and biases of convolution\n",
    "                     and batch normalization layers\n",
    "                It's an `OrderedDict`\n",
    "        \"\"\"\n",
    "        for block in [1, 2, 3, 4]:\n",
    "            x = ConvBlockFunction(\n",
    "                x,\n",
    "                params[f\"conv{block}.0.weight\"],\n",
    "                params[f\"conv{block}.0.bias\"],\n",
    "                params.get(f\"conv{block}.1.weight\"),\n",
    "                params.get(f\"conv{block}.1.bias\"),\n",
    "            )\n",
    "        x = x.view(x.shape[0], -1)\n",
    "        x = F.linear(x, params[\"logits.weight\"], params[\"logits.bias\"])\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-03T07:53:14.347435Z",
     "iopub.status.busy": "2025-07-03T07:53:14.346827Z",
     "iopub.status.idle": "2025-07-03T07:53:14.378377Z",
     "shell.execute_reply": "2025-07-03T07:53:14.377796Z",
     "shell.execute_reply.started": "2025-07-03T07:53:14.347406Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 0, 1, 1, 2, 2, 3, 3, 4, 4])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def create_label(n_way, k_shot):\n",
    "    return torch.arange(n_way).repeat_interleave(k_shot).long()\n",
    "\n",
    "\n",
    "# Try to create labels for 5-way 2-shot setting\n",
    "create_label(5, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-03T07:53:16.002446Z",
     "iopub.status.busy": "2025-07-03T07:53:16.001736Z",
     "iopub.status.idle": "2025-07-03T07:53:16.006554Z",
     "shell.execute_reply": "2025-07-03T07:53:16.005861Z",
     "shell.execute_reply.started": "2025-07-03T07:53:16.002418Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def calculate_accuracy(logits, labels):\n",
    "    \"\"\"utility function for accuracy calculation\"\"\"\n",
    "    acc = np.asarray(\n",
    "        [(torch.argmax(logits, -1).cpu().numpy() == labels.cpu().numpy())]\n",
    "    ).mean()\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-03T07:53:27.449961Z",
     "iopub.status.busy": "2025-07-03T07:53:27.449503Z",
     "iopub.status.idle": "2025-07-03T07:53:27.455915Z",
     "shell.execute_reply": "2025-07-03T07:53:27.455293Z",
     "shell.execute_reply.started": "2025-07-03T07:53:27.449936Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Dataset for train and val\n",
    "class Omniglot(Dataset):\n",
    "    def __init__(self, data_dir, k_shot, q_query, task_num=None):\n",
    "        self.file_list = [\n",
    "            f for f in glob.glob(data_dir + \"**/character*\", recursive=True)\n",
    "        ]\n",
    "        # limit task number if task_num is set\n",
    "        if task_num is not None:\n",
    "            self.file_list = self.file_list[: min(len(self.file_list), task_num)]\n",
    "        self.transform = transforms.Compose([transforms.ToTensor()])\n",
    "        self.n = k_shot + q_query\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # For random sampling the characters we want.\n",
    "        img_path = self.file_list[idx]\n",
    "        img_list = [f for f in glob.glob(img_path + \"**/*.png\", recursive=True)]\n",
    "        img_list.sort()\n",
    "        \n",
    "        sample = np.arange(len(img_list))\n",
    "        np.random.shuffle(sample)\n",
    "        \n",
    "        # `k_shot + q_query` examples for each character\n",
    "        imgs = [self.transform(Image.open(img_list[idx])) for idx in sample[:self.n]]\n",
    "        imgs = torch.stack(imgs)\n",
    "        return imgs\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-03T07:53:41.288122Z",
     "iopub.status.busy": "2025-07-03T07:53:41.287303Z",
     "iopub.status.idle": "2025-07-03T07:53:41.296694Z",
     "shell.execute_reply": "2025-07-03T07:53:41.296039Z",
     "shell.execute_reply.started": "2025-07-03T07:53:41.288093Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def BaseSolver(\n",
    "    model,\n",
    "    optimizer,\n",
    "    x,\n",
    "    n_way,\n",
    "    k_shot,\n",
    "    q_query,\n",
    "    loss_fn,\n",
    "    inner_train_step=1,\n",
    "    inner_lr=0.4,\n",
    "    train=True,\n",
    "    return_labels=False,\n",
    "):\n",
    "    criterion, task_loss, task_acc = loss_fn, [], []\n",
    "    labels = []\n",
    "\n",
    "    for meta_batch in x:\n",
    "        # Get data\n",
    "        support_set = meta_batch[: n_way * k_shot]\n",
    "        query_set = meta_batch[n_way * k_shot :]\n",
    "\n",
    "        if train:\n",
    "            \"\"\" training loop \"\"\"\n",
    "            # Use the support set to calculate loss\n",
    "            labels = create_label(n_way, k_shot).to(device)\n",
    "            logits = model.forward(support_set)\n",
    "            loss = criterion(logits, labels)\n",
    "\n",
    "            task_loss.append(loss)\n",
    "            task_acc.append(calculate_accuracy(logits, labels))\n",
    "        else:\n",
    "            \"\"\" validation / testing loop \"\"\"\n",
    "            # First update model with support set images for `inner_train_step` steps\n",
    "            fast_weights = OrderedDict(model.named_parameters())\n",
    "\n",
    "\n",
    "            for inner_step in range(inner_train_step):\n",
    "                # Simply training\n",
    "                train_label = create_label(n_way, k_shot).to(device)\n",
    "                logits = model.functional_forward(support_set, fast_weights)\n",
    "                loss = criterion(logits, train_label)\n",
    "\n",
    "                grads = torch.autograd.grad(loss, fast_weights.values(), create_graph=True)\n",
    "                # Perform SGD\n",
    "                fast_weights = OrderedDict(\n",
    "                    (name, param - inner_lr * grad)\n",
    "                    for ((name, param), grad) in zip(fast_weights.items(), grads)\n",
    "                )\n",
    "\n",
    "            if not return_labels:\n",
    "                \"\"\" validation \"\"\"\n",
    "                val_label = create_label(n_way, q_query).to(device)\n",
    "\n",
    "                logits = model.functional_forward(query_set, fast_weights)\n",
    "                loss = criterion(logits, val_label)\n",
    "                task_loss.append(loss)\n",
    "                task_acc.append(calculate_accuracy(logits, val_label))\n",
    "            else:\n",
    "                \"\"\" testing \"\"\"\n",
    "                logits = model.functional_forward(query_set, fast_weights)\n",
    "                labels.extend(torch.argmax(logits, -1).cpu().numpy())\n",
    "\n",
    "    if return_labels:\n",
    "        return labels\n",
    "\n",
    "    batch_loss = torch.stack(task_loss).mean()\n",
    "    task_acc = np.mean(task_acc)\n",
    "\n",
    "    if train:\n",
    "        # Update model\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        batch_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    return batch_loss, task_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-03T07:53:49.495706Z",
     "iopub.status.busy": "2025-07-03T07:53:49.495025Z",
     "iopub.status.idle": "2025-07-03T07:53:49.503659Z",
     "shell.execute_reply": "2025-07-03T07:53:49.503098Z",
     "shell.execute_reply.started": "2025-07-03T07:53:49.495680Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def MetaSolver(\n",
    "    model,\n",
    "    optimizer,\n",
    "    x,\n",
    "    n_way,\n",
    "    k_shot,\n",
    "    q_query,\n",
    "    loss_fn,\n",
    "    inner_train_step=1,\n",
    "    inner_lr=0.4,\n",
    "    train=True,\n",
    "    return_labels=False\n",
    "):\n",
    "    criterion, task_loss, task_acc = loss_fn, [], []\n",
    "    labels = []\n",
    "\n",
    "    for meta_batch in x:\n",
    "        # Get data\n",
    "        support_set = meta_batch[: n_way * k_shot]\n",
    "        query_set = meta_batch[n_way * k_shot :]\n",
    "\n",
    "        # Copy the params for inner loop\n",
    "        fast_weights = OrderedDict(model.named_parameters())\n",
    "\n",
    "        ### ---------- INNER TRAIN LOOP ---------- ###\n",
    "        for inner_step in range(inner_train_step):\n",
    "            # Simply training\n",
    "            train_label = create_label(n_way, k_shot).to(device)\n",
    "            logits = model.functional_forward(support_set, fast_weights)\n",
    "            loss = criterion(logits, train_label)\n",
    "            # Inner gradients update! vvvvvvvvvvvvvvvvvvvv #\n",
    "            \"\"\" Inner Loop Update \"\"\"\n",
    "            # TODO: Finish the inner loop update rule\n",
    "            raise NotImplementedError\n",
    "            # ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ #\n",
    "\n",
    "        ### ---------- INNER VALID LOOP ---------- ###\n",
    "        if not return_labels:\n",
    "            \"\"\" training / validation \"\"\"\n",
    "            val_label = create_label(n_way, q_query).to(device)\n",
    "\n",
    "            # Collect gradients for outer loop\n",
    "            logits = model.functional_forward(query_set, fast_weights)\n",
    "            loss = criterion(logits, val_label)\n",
    "            task_loss.append(loss)\n",
    "            task_acc.append(calculate_accuracy(logits, val_label))\n",
    "        else:\n",
    "            \"\"\" testing \"\"\"\n",
    "            logits = model.functional_forward(query_set, fast_weights)\n",
    "            labels.extend(torch.argmax(logits, -1).cpu().numpy())\n",
    "\n",
    "    if return_labels:\n",
    "        return labels\n",
    "\n",
    "    # Update outer loop\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    meta_batch_loss = torch.stack(task_loss).mean()\n",
    "    if train:\n",
    "        \"\"\" Outer Loop Update \"\"\"\n",
    "        # TODO: Finish the outer loop update\n",
    "        raise NotimplementedError\n",
    "\n",
    "    task_acc = np.mean(task_acc)\n",
    "    return meta_batch_loss, task_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-03T07:54:17.297015Z",
     "iopub.status.busy": "2025-07-03T07:54:17.296546Z",
     "iopub.status.idle": "2025-07-03T07:54:17.301173Z",
     "shell.execute_reply": "2025-07-03T07:54:17.300479Z",
     "shell.execute_reply.started": "2025-07-03T07:54:17.296990Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "n_way = 5\n",
    "k_shot = 1\n",
    "q_query = 1\n",
    "train_inner_train_step = 1\n",
    "val_inner_train_step = 3\n",
    "inner_lr = 0.4\n",
    "meta_lr = 0.001\n",
    "meta_batch_size = 32\n",
    "max_epoch = 30\n",
    "eval_batches = 20\n",
    "train_data_path = \"/kaggle/working/Omniglot/images_background\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-03T07:54:28.993630Z",
     "iopub.status.busy": "2025-07-03T07:54:28.993357Z",
     "iopub.status.idle": "2025-07-03T07:54:28.998706Z",
     "shell.execute_reply": "2025-07-03T07:54:28.997863Z",
     "shell.execute_reply.started": "2025-07-03T07:54:28.993611Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def dataloader_init(datasets, shuffle=True, num_workers=2):\n",
    "    train_set, val_set = datasets\n",
    "    train_loader = DataLoader(\n",
    "        train_set,\n",
    "        # The \"batch_size\" here is not \\\n",
    "        #    the meta batch size, but  \\\n",
    "        #    how many different        \\\n",
    "        #    characters in a task,     \\\n",
    "        #    i.e. the \"n_way\" in       \\\n",
    "        #    few-shot classification.\n",
    "        batch_size=n_way,\n",
    "        num_workers=num_workers,\n",
    "        shuffle=shuffle,\n",
    "        drop_last=True,\n",
    "    )\n",
    "    val_loader = DataLoader(\n",
    "        val_set, batch_size=n_way, num_workers=num_workers, shuffle=shuffle, drop_last=True\n",
    "    )\n",
    "\n",
    "    train_iter = iter(train_loader)\n",
    "    val_iter = iter(val_loader)\n",
    "    return (train_loader, val_loader), (train_iter, val_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-03T07:54:34.770598Z",
     "iopub.status.busy": "2025-07-03T07:54:34.770325Z",
     "iopub.status.idle": "2025-07-03T07:54:34.774955Z",
     "shell.execute_reply": "2025-07-03T07:54:34.774171Z",
     "shell.execute_reply.started": "2025-07-03T07:54:34.770577Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def model_init():\n",
    "    meta_model = Classifier(1, n_way).to(device)\n",
    "    optimizer = torch.optim.Adam(meta_model.parameters(), lr=meta_lr)\n",
    "    loss_fn = nn.CrossEntropyLoss().to(device)\n",
    "    return meta_model, optimizer, loss_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-03T07:54:39.261921Z",
     "iopub.status.busy": "2025-07-03T07:54:39.261252Z",
     "iopub.status.idle": "2025-07-03T07:54:39.266633Z",
     "shell.execute_reply": "2025-07-03T07:54:39.266008Z",
     "shell.execute_reply.started": "2025-07-03T07:54:39.261899Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def get_meta_batch(meta_batch_size, k_shot, q_query, data_loader, iterator):\n",
    "    data = []\n",
    "    for _ in range(meta_batch_size):\n",
    "        try:\n",
    "            # a \"task_data\" tensor is representing \\\n",
    "            #     the data of a task, with size of \\\n",
    "            #     [n_way, k_shot+q_query, 1, 28, 28]\n",
    "            task_data = next(iterator)\n",
    "        except StopIteration:\n",
    "            iterator = iter(data_loader)\n",
    "            task_data = next(iterator)\n",
    "        train_data = task_data[:, :k_shot].reshape(-1, 1, 28, 28)\n",
    "        val_data = task_data[:, k_shot:].reshape(-1, 1, 28, 28)\n",
    "        task_data = torch.cat((train_data, val_data), 0)\n",
    "        data.append(task_data)\n",
    "    return torch.stack(data).to(device), iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2025-07-03T07:59:31.074407Z",
     "iopub.status.busy": "2025-07-03T07:59:31.073600Z",
     "iopub.status.idle": "2025-07-03T07:59:32.142714Z",
     "shell.execute_reply": "2025-07-03T07:59:32.141669Z",
     "shell.execute_reply.started": "2025-07-03T07:59:31.074371Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "数据集实际长度: 0\n",
      "\n",
      "第 1 轮训练\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a909928011c34cc9a640ab1add9caffd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "训练进度:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "警告: 数据迭代器已耗尽，重新初始化...\n",
      "  平均损失: nan\t  准确率: nan%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/numpy/core/fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/usr/local/lib/python3.11/dist-packages/numpy/core/_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "274ea000d559463ba8512d3301c55f26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "验证进度:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "警告: 验证数据迭代器已耗尽，重新初始化...\n",
      "  验证准确率: nan%\n",
      "\n",
      "第 2 轮训练\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6df2c766b53a46d0a3ddade1ed0c915a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "训练进度:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "警告: 数据迭代器已耗尽，重新初始化...\n",
      "  平均损失: nan\t  准确率: nan%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d38c3964e42e4691933c0f71f3cc2742",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "验证进度:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "警告: 验证数据迭代器已耗尽，重新初始化...\n",
      "  验证准确率: nan%\n",
      "\n",
      "第 3 轮训练\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3d47e1cc5834ca5bf6f3daefd981a41",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "训练进度:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "警告: 数据迭代器已耗尽，重新初始化...\n",
      "  平均损失: nan\t  准确率: nan%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac6ad849042f40238e554373b69c20f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "验证进度:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "警告: 验证数据迭代器已耗尽，重新初始化...\n",
      "  验证准确率: nan%\n",
      "\n",
      "第 4 轮训练\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bed8a139aa6340c88861cb5bd203959a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "训练进度:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "警告: 数据迭代器已耗尽，重新初始化...\n",
      "  平均损失: nan\t  准确率: nan%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8376fabb178a4947bd3499dac084ff06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "验证进度:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "警告: 验证数据迭代器已耗尽，重新初始化...\n",
      "  验证准确率: nan%\n",
      "\n",
      "第 5 轮训练\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74bde6f261114815b5d94109baf559e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "训练进度:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "警告: 数据迭代器已耗尽，重新初始化...\n",
      "  平均损失: nan\t  准确率: nan%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94a28f8c94b447e381c65e791481d9e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "验证进度:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "警告: 验证数据迭代器已耗尽，重新初始化...\n",
      "  验证准确率: nan%\n"
     ]
    }
   ],
   "source": [
    "# 设置求解器类型（基础版或元学习版）\n",
    "solver = 'base'  # 可选 'base' 或 'meta'\n",
    "meta_model, optimizer, loss_fn = model_init()\n",
    "\n",
    "# 根据求解器类型初始化数据集和加载器\n",
    "if solver == 'base':\n",
    "    max_epoch = 5  # 基础求解器只需要5个epoch\n",
    "    Solver = BaseSolver\n",
    "    \n",
    "    # 创建数据集并检查实际长度\n",
    "    dataset = Omniglot(train_data_path, k_shot, q_query, task_num=10)\n",
    "    actual_length = len(dataset)\n",
    "    print(f\"数据集实际长度: {actual_length}\")  # 调试信息\n",
    "    \n",
    "    # 确保训练集和验证集分割正确\n",
    "    if actual_length >= 10:  # 如果数据集足够大\n",
    "        train_size = 8  # 训练集大小（可调整）\n",
    "        val_size = 2    # 验证集大小\n",
    "    else:  # 如果数据集比预期小\n",
    "        train_size = int(0.8 * actual_length)  # 80%训练\n",
    "        val_size = actual_length - train_size  # 20%验证\n",
    "    \n",
    "    # 随机分割数据集\n",
    "    train_set, val_set = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
    "    \n",
    "    # 初始化数据加载器\n",
    "    (train_loader, val_loader), (train_iter, val_iter) = dataloader_init(\n",
    "        (train_set, val_set), \n",
    "        shuffle=False  # 不打乱顺序\n",
    "    )\n",
    "\n",
    "elif solver == 'meta':\n",
    "    Solver = MetaSolver\n",
    "    dataset = Omniglot(train_data_path, k_shot, q_query)\n",
    "    train_split = int(0.8 * len(dataset))  # 80%训练\n",
    "    val_split = len(dataset) - train_split  # 20%验证\n",
    "    train_set, val_set = torch.utils.data.random_split(dataset, [train_split, val_split])\n",
    "    (train_loader, val_loader), (train_iter, val_iter) = dataloader_init((train_set, val_set))\n",
    "else:\n",
    "    raise NotImplementedError(\"不支持的求解器类型\")\n",
    "\n",
    "# 主训练循环\n",
    "for epoch in range(max_epoch):\n",
    "    print(f\"\\n第 {epoch + 1} 轮训练\")\n",
    "    train_meta_loss = []\n",
    "    train_acc = []\n",
    "    \n",
    "    # 使用tqdm显示进度条\n",
    "    for step in tqdm(range(max(1, len(train_loader) // meta_batch_size)), desc=\"训练进度\"):\n",
    "        try:\n",
    "            # 获取一个元批量的数据\n",
    "            x, train_iter = get_meta_batch(\n",
    "                meta_batch_size, \n",
    "                k_shot, \n",
    "                q_query, \n",
    "                train_loader, \n",
    "                train_iter\n",
    "            )\n",
    "            \n",
    "            # 使用求解器训练\n",
    "            meta_loss, acc = Solver(\n",
    "                meta_model,\n",
    "                optimizer,\n",
    "                x,\n",
    "                n_way,\n",
    "                k_shot,\n",
    "                q_query,\n",
    "                loss_fn, \n",
    "                inner_train_step=train_inner_train_step\n",
    "            )\n",
    "            \n",
    "            train_meta_loss.append(meta_loss.item())\n",
    "            train_acc.append(acc)\n",
    "            \n",
    "        except StopIteration:\n",
    "            print(\"\\n警告: 数据迭代器已耗尽，重新初始化...\")\n",
    "            train_iter = iter(train_loader)\n",
    "            continue\n",
    "    \n",
    "    # 打印训练结果\n",
    "    print(\"  平均损失: %.3f\" % np.mean(train_meta_loss), end=\"\\t\")\n",
    "    print(\"  准确率: %.2f%%\" % (np.mean(train_acc) * 100))\n",
    "\n",
    "    # 验证阶段\n",
    "    val_acc = []\n",
    "    for eval_step in tqdm(range(max(1, len(val_loader) // eval_batches)), desc=\"验证进度\"):\n",
    "        try:\n",
    "            x, val_iter = get_meta_batch(\n",
    "                eval_batches, \n",
    "                k_shot, \n",
    "                q_query, \n",
    "                val_loader, \n",
    "                val_iter\n",
    "            )\n",
    "            _, acc = Solver(\n",
    "                meta_model,\n",
    "                optimizer,\n",
    "                x,\n",
    "                n_way,\n",
    "                k_shot,\n",
    "                q_query,\n",
    "                loss_fn,\n",
    "                inner_train_step=val_inner_train_step,\n",
    "                train=False,  # 验证模式\n",
    "            )\n",
    "            val_acc.append(acc)\n",
    "            \n",
    "        except StopIteration:\n",
    "            print(\"\\n警告: 验证数据迭代器已耗尽，重新初始化...\")\n",
    "            val_iter = iter(val_loader)\n",
    "            continue\n",
    "    \n",
    "    print(\"  验证准确率: %.2f%%\" % (np.mean(val_acc) * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-03T07:59:58.290321Z",
     "iopub.status.busy": "2025-07-03T07:59:58.290035Z",
     "iopub.status.idle": "2025-07-03T07:59:58.296359Z",
     "shell.execute_reply": "2025-07-03T07:59:58.295599Z",
     "shell.execute_reply.started": "2025-07-03T07:59:58.290299Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# test dataset\n",
    "class OmniglotTest(Dataset):\n",
    "    def __init__(self, test_dir):\n",
    "        self.test_dir = test_dir\n",
    "        self.n = 5\n",
    "\n",
    "        self.transform = transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        support_files = [\n",
    "            os.path.join(self.test_dir, \"support\", f\"{idx:>04}\", f\"image_{i}.png\")\n",
    "            for i in range(self.n)\n",
    "        ]\n",
    "        query_files = [\n",
    "            os.path.join(self.test_dir, \"query\", f\"{idx:>04}\", f\"image_{i}.png\")\n",
    "            for i in range(self.n)\n",
    "        ]\n",
    "\n",
    "        support_imgs = torch.stack(\n",
    "            [self.transform(Image.open(e)) for e in support_files]\n",
    "        )\n",
    "        query_imgs = torch.stack([self.transform(Image.open(e)) for e in query_files])\n",
    "\n",
    "        return support_imgs, query_imgs\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(os.listdir(os.path.join(self.test_dir, \"support\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-03T08:00:03.706105Z",
     "iopub.status.busy": "2025-07-03T08:00:03.705625Z",
     "iopub.status.idle": "2025-07-03T08:00:25.905606Z",
     "shell.execute_reply": "2025-07-03T08:00:25.904960Z",
     "shell.execute_reply.started": "2025-07-03T08:00:03.706082Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95ccab2e560f43aeb5b82f5f189f61a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_inner_train_step = 10 # you can change this\n",
    "\n",
    "test_batches = 20\n",
    "test_dataset = OmniglotTest(\"Omniglot-test\")\n",
    "test_loader = DataLoader(test_dataset, batch_size=test_batches, shuffle=False)\n",
    "\n",
    "output = []\n",
    "for _, batch in enumerate(tqdm(test_loader)):\n",
    "    support_set, query_set = batch\n",
    "    x = torch.cat([support_set, query_set], dim=1)\n",
    "    x = x.to(device)\n",
    "\n",
    "    labels = Solver(\n",
    "        meta_model,\n",
    "        optimizer,\n",
    "        x,\n",
    "        n_way,\n",
    "        k_shot,\n",
    "        q_query,\n",
    "        loss_fn,\n",
    "        inner_train_step=test_inner_train_step,\n",
    "        train=False,\n",
    "        return_labels=True,\n",
    "    )\n",
    "\n",
    "    output.extend(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-03T08:00:28.076274Z",
     "iopub.status.busy": "2025-07-03T08:00:28.075963Z",
     "iopub.status.idle": "2025-07-03T08:00:28.082797Z",
     "shell.execute_reply": "2025-07-03T08:00:28.082182Z",
     "shell.execute_reply.started": "2025-07-03T08:00:28.076252Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# write to csv\n",
    "with open(\"output.csv\", \"w\") as f:\n",
    "    f.write(f\"id,class\\n\")\n",
    "    for i, label in enumerate(output):\n",
    "        f.write(f\"{i},{label}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
