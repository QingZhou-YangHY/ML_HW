{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Meta Learning : Learn how to learn\n## Machine Learning = Looking for a function\n## Step 1: Function with unknown\n## Step 2: Define loss function\n## Step 3: Optimization\n\n## Learning algorithm:Hand-crafted\n## 机器自己学出来的就是learnable components\n\n## Meta Learning是跨任务学习Across-task Training,Machine Learning是Within-task Testing","metadata":{}},{"cell_type":"code","source":"workspace_dir = '/kaggle/working/'\n\n# Download dataset\n!wget https://www.dropbox.com/s/pqeym3n4jly5e89/Omniglot.tar.gz?dl=1 \\\n    -O \"{workspace_dir}/Omniglot.tar.gz\"\n!wget https://www.dropbox.com/s/nlvokertmksfc42/Omniglot-test.tar.gz?dl=1 \\\n    -O \"{workspace_dir}/Omniglot-test.tar.gz\"\n\n# Use `tar' command to decompress\n!tar -zxf \"{workspace_dir}/Omniglot.tar.gz\" -C \"{workspace_dir}/\"\n!tar -zxf \"{workspace_dir}/Omniglot-test.tar.gz\" -C \"{workspace_dir}/\"","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-07-03T07:52:26.234788Z","iopub.execute_input":"2025-07-03T07:52:26.234950Z","iopub.status.idle":"2025-07-03T07:52:31.523115Z","shell.execute_reply.started":"2025-07-03T07:52:26.234934Z","shell.execute_reply":"2025-07-03T07:52:31.522306Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"name":"stdout","text":"--2025-07-03 07:52:26--  https://www.dropbox.com/s/pqeym3n4jly5e89/Omniglot.tar.gz?dl=1\nResolving www.dropbox.com (www.dropbox.com)... 162.125.3.18, 2620:100:6018:18::a27d:312\nConnecting to www.dropbox.com (www.dropbox.com)|162.125.3.18|:443... connected.\nHTTP request sent, awaiting response... 302 Found\nLocation: https://www.dropbox.com/scl/fi/peae3bis6c9i96zsmmbzc/Omniglot.tar.gz?rlkey=v9ljhktg1wiy3x9otdz3p7k8c&dl=1 [following]\n--2025-07-03 07:52:26--  https://www.dropbox.com/scl/fi/peae3bis6c9i96zsmmbzc/Omniglot.tar.gz?rlkey=v9ljhktg1wiy3x9otdz3p7k8c&dl=1\nReusing existing connection to www.dropbox.com:443.\nHTTP request sent, awaiting response... 302 Found\nLocation: https://ucc8b4901ceb6b316a8bbf2bb94c.dl.dropboxusercontent.com/cd/0/inline/Csx3YKOvieFHZWQeYexq-yQSQXgGPQ8RkBrJn9CRnG0U-lDcuiFOYMt-ZQTbN1RP6l8xzEOD9VQ54O-BaWPISvo-IQAYFXmOO-F6C3RzYMb4N81nMl4yDaIn-HMNfF0H0J-_4ZHtqMMLIRxxDVwrfIlt/file?dl=1# [following]\n--2025-07-03 07:52:26--  https://ucc8b4901ceb6b316a8bbf2bb94c.dl.dropboxusercontent.com/cd/0/inline/Csx3YKOvieFHZWQeYexq-yQSQXgGPQ8RkBrJn9CRnG0U-lDcuiFOYMt-ZQTbN1RP6l8xzEOD9VQ54O-BaWPISvo-IQAYFXmOO-F6C3RzYMb4N81nMl4yDaIn-HMNfF0H0J-_4ZHtqMMLIRxxDVwrfIlt/file?dl=1\nResolving ucc8b4901ceb6b316a8bbf2bb94c.dl.dropboxusercontent.com (ucc8b4901ceb6b316a8bbf2bb94c.dl.dropboxusercontent.com)... 162.125.3.15, 2620:100:6018:15::a27d:30f\nConnecting to ucc8b4901ceb6b316a8bbf2bb94c.dl.dropboxusercontent.com (ucc8b4901ceb6b316a8bbf2bb94c.dl.dropboxusercontent.com)|162.125.3.15|:443... connected.\nHTTP request sent, awaiting response... 302 Found\nLocation: /cd/0/inline2/Cswr8lp_snJoMBWD_zRJCWWvHk2ZksS-D_DkbsGwmgHnMZqB7VAYHX2_5mS6Q_hshhKq3ovhEc4EkpplknjXR-GnVpgr2JTlSiFw46uesK1KBr7oLSA_ePP9pMCHyH03CJSteOvMd3IavB0Fgm6CePPHIdpB9ZozDI6ztm3PLhfQpmLkBkb4_CGcQCC0ZvKlxYp5qtgHaLacApL-GYNP4ELXHybHWgu4cfndqhVI22s0o7wIx8RLeAY9jM4VROpmd48t44TsgDnPGlRczJvdWnWsAoZyxFQSXRKZ10sdv-2E6SLApObYewhvsx6hxZQADsoYW-XK40V2mvLKlGVKkdmldn4-34CwTzSPI6-ZH9a3CrVWd8e5Kne7VtbNN49iEaw/file?dl=1 [following]\n--2025-07-03 07:52:27--  https://ucc8b4901ceb6b316a8bbf2bb94c.dl.dropboxusercontent.com/cd/0/inline2/Cswr8lp_snJoMBWD_zRJCWWvHk2ZksS-D_DkbsGwmgHnMZqB7VAYHX2_5mS6Q_hshhKq3ovhEc4EkpplknjXR-GnVpgr2JTlSiFw46uesK1KBr7oLSA_ePP9pMCHyH03CJSteOvMd3IavB0Fgm6CePPHIdpB9ZozDI6ztm3PLhfQpmLkBkb4_CGcQCC0ZvKlxYp5qtgHaLacApL-GYNP4ELXHybHWgu4cfndqhVI22s0o7wIx8RLeAY9jM4VROpmd48t44TsgDnPGlRczJvdWnWsAoZyxFQSXRKZ10sdv-2E6SLApObYewhvsx6hxZQADsoYW-XK40V2mvLKlGVKkdmldn4-34CwTzSPI6-ZH9a3CrVWd8e5Kne7VtbNN49iEaw/file?dl=1\nReusing existing connection to ucc8b4901ceb6b316a8bbf2bb94c.dl.dropboxusercontent.com:443.\nHTTP request sent, awaiting response... 200 OK\nLength: 5718170 (5.5M) [application/binary]\nSaving to: ‘/kaggle/working//Omniglot.tar.gz’\n\n/kaggle/working//Om 100%[===================>]   5.45M  --.-KB/s    in 0.09s   \n\n2025-07-03 07:52:27 (62.5 MB/s) - ‘/kaggle/working//Omniglot.tar.gz’ saved [5718170/5718170]\n\n--2025-07-03 07:52:27--  https://www.dropbox.com/s/nlvokertmksfc42/Omniglot-test.tar.gz?dl=1\nResolving www.dropbox.com (www.dropbox.com)... 162.125.3.18, 2620:100:6018:18::a27d:312\nConnecting to www.dropbox.com (www.dropbox.com)|162.125.3.18|:443... connected.\nHTTP request sent, awaiting response... 302 Found\nLocation: https://www.dropbox.com/scl/fi/7y3ivzl5b8oq5ogj7pfq9/Omniglot-test.tar.gz?rlkey=cm9x0et2xwfl9trijmwkh0qz0&dl=1 [following]\n--2025-07-03 07:52:28--  https://www.dropbox.com/scl/fi/7y3ivzl5b8oq5ogj7pfq9/Omniglot-test.tar.gz?rlkey=cm9x0et2xwfl9trijmwkh0qz0&dl=1\nReusing existing connection to www.dropbox.com:443.\nHTTP request sent, awaiting response... 302 Found\nLocation: https://ucb168d8aa96a02d858064147aec.dl.dropboxusercontent.com/cd/0/inline/CsxJ68gz27LIHip1gt84obfQU6Z5P-9GQzHWVd_uXa7_-uyR4dahbzSpK793a-S0nQtQsDk6FDccutKvgf9kzB0IZ2wBuqb-mELIzfE0b_jkybbfxWvtF_HwFmmQlW8MH0Nwq3AswK4w_jmuvQKExu1P/file?dl=1# [following]\n--2025-07-03 07:52:28--  https://ucb168d8aa96a02d858064147aec.dl.dropboxusercontent.com/cd/0/inline/CsxJ68gz27LIHip1gt84obfQU6Z5P-9GQzHWVd_uXa7_-uyR4dahbzSpK793a-S0nQtQsDk6FDccutKvgf9kzB0IZ2wBuqb-mELIzfE0b_jkybbfxWvtF_HwFmmQlW8MH0Nwq3AswK4w_jmuvQKExu1P/file?dl=1\nResolving ucb168d8aa96a02d858064147aec.dl.dropboxusercontent.com (ucb168d8aa96a02d858064147aec.dl.dropboxusercontent.com)... 162.125.3.15, 2620:100:6018:15::a27d:30f\nConnecting to ucb168d8aa96a02d858064147aec.dl.dropboxusercontent.com (ucb168d8aa96a02d858064147aec.dl.dropboxusercontent.com)|162.125.3.15|:443... connected.\nHTTP request sent, awaiting response... 302 Found\nLocation: /cd/0/inline2/Csyb5PPjdj2KH6EeO4ya9vxFxTFcW8PuUdfAQOzh1RE4vzkF1XgAro5CqOpjBTvoZly2KHd3C_Zo3BOZvhS05X61q2tvRU_pTmS51DTD_mtHLN1Z-73EQkx38hTkGA2JygeZqf9huoIQFe0oK8vVgBna1FaLuzqUJrpMNtIfzKmhhvrHZmCFUFIWhhuXu1oYj-W0RKTtc6coOe00DdgYU7AEWKRJOKXVLZbGoMzJAL87Iq71LkCEj8m1r7R-RFh52o2Xkv6j4AnvNXal_Sb9dUaExOomjtTCeKQNVT8s4fh6LmAzmZYLZ80aZyJw1LxhqVu6-9B-n_J0x9tnfjWMtshPUGKajjANRaooKKOux1AaPoa9_UwVP43BRm-B_ROLWWU/file?dl=1 [following]\n--2025-07-03 07:52:29--  https://ucb168d8aa96a02d858064147aec.dl.dropboxusercontent.com/cd/0/inline2/Csyb5PPjdj2KH6EeO4ya9vxFxTFcW8PuUdfAQOzh1RE4vzkF1XgAro5CqOpjBTvoZly2KHd3C_Zo3BOZvhS05X61q2tvRU_pTmS51DTD_mtHLN1Z-73EQkx38hTkGA2JygeZqf9huoIQFe0oK8vVgBna1FaLuzqUJrpMNtIfzKmhhvrHZmCFUFIWhhuXu1oYj-W0RKTtc6coOe00DdgYU7AEWKRJOKXVLZbGoMzJAL87Iq71LkCEj8m1r7R-RFh52o2Xkv6j4AnvNXal_Sb9dUaExOomjtTCeKQNVT8s4fh6LmAzmZYLZ80aZyJw1LxhqVu6-9B-n_J0x9tnfjWMtshPUGKajjANRaooKKOux1AaPoa9_UwVP43BRm-B_ROLWWU/file?dl=1\nReusing existing connection to ucb168d8aa96a02d858064147aec.dl.dropboxusercontent.com:443.\nHTTP request sent, awaiting response... 200 OK\nLength: 1822122 (1.7M) [application/binary]\nSaving to: ‘/kaggle/working//Omniglot-test.tar.gz’\n\n/kaggle/working//Om 100%[===================>]   1.74M  --.-KB/s    in 0.07s   \n\n2025-07-03 07:52:29 (25.8 MB/s) - ‘/kaggle/working//Omniglot-test.tar.gz’ saved [1822122/1822122]\n\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# Import modules we need\nimport glob, random\nfrom collections import OrderedDict\n\nimport numpy as np\nfrom tqdm.auto import tqdm\n\nimport torch, torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader, Dataset\nimport torchvision.transforms as transforms\n\nfrom PIL import Image\nfrom IPython.display import display\n\n# Check device\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprint(f\"DEVICE = {device}\")\n\n# Fix random seeds\nrandom_seed = 0\nrandom.seed(random_seed)\nnp.random.seed(random_seed)\ntorch.manual_seed(random_seed)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed_all(random_seed)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-03T07:52:55.882552Z","iopub.execute_input":"2025-07-03T07:52:55.883131Z","iopub.status.idle":"2025-07-03T07:53:02.439080Z","shell.execute_reply.started":"2025-07-03T07:52:55.883086Z","shell.execute_reply":"2025-07-03T07:53:02.438488Z"}},"outputs":[{"name":"stdout","text":"DEVICE = cuda\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"def ConvBlock(in_ch: int, out_ch: int):\n    return nn.Sequential(\n        nn.Conv2d(in_ch, out_ch, 3, padding=1),\n        nn.BatchNorm2d(out_ch),\n        nn.ReLU(),\n        nn.MaxPool2d(kernel_size=2, stride=2),\n    )\n\n\ndef ConvBlockFunction(x, w, b, w_bn, b_bn):\n    x = F.conv2d(x, w, b, padding=1)\n    x = F.batch_norm(\n        x, running_mean=None, running_var=None, weight=w_bn, bias=b_bn, training=True\n    )\n    x = F.relu(x)\n    x = F.max_pool2d(x, kernel_size=2, stride=2)\n    return x","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-03T07:53:06.703730Z","iopub.execute_input":"2025-07-03T07:53:06.703999Z","iopub.status.idle":"2025-07-03T07:53:06.709160Z","shell.execute_reply.started":"2025-07-03T07:53:06.703977Z","shell.execute_reply":"2025-07-03T07:53:06.708602Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"class Classifier(nn.Module):\n    def __init__(self, in_ch, k_way):\n        super(Classifier, self).__init__()\n        self.conv1 = ConvBlock(in_ch, 64)\n        self.conv2 = ConvBlock(64, 64)\n        self.conv3 = ConvBlock(64, 64)\n        self.conv4 = ConvBlock(64, 64)\n        self.logits = nn.Linear(64, k_way)\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.conv2(x)\n        x = self.conv3(x)\n        x = self.conv4(x)\n        x = x.view(x.shape[0], -1)\n        x = self.logits(x)\n        return x\n\n    def functional_forward(self, x, params):\n        \"\"\"\n        Arguments:\n        x: input images [batch, 1, 28, 28]\n        params: model parameters,\n                i.e. weights and biases of convolution\n                     and weights and biases of\n                                   batch normalization\n                type is an OrderedDict\n\n        Arguments:\n        x: input images [batch, 1, 28, 28]\n        params: The model parameters,\n                i.e. weights and biases of convolution\n                     and batch normalization layers\n                It's an `OrderedDict`\n        \"\"\"\n        for block in [1, 2, 3, 4]:\n            x = ConvBlockFunction(\n                x,\n                params[f\"conv{block}.0.weight\"],\n                params[f\"conv{block}.0.bias\"],\n                params.get(f\"conv{block}.1.weight\"),\n                params.get(f\"conv{block}.1.bias\"),\n            )\n        x = x.view(x.shape[0], -1)\n        x = F.linear(x, params[\"logits.weight\"], params[\"logits.bias\"])\n        return x","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-03T07:53:11.824279Z","iopub.execute_input":"2025-07-03T07:53:11.825035Z","iopub.status.idle":"2025-07-03T07:53:11.831722Z","shell.execute_reply.started":"2025-07-03T07:53:11.825003Z","shell.execute_reply":"2025-07-03T07:53:11.830948Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"def create_label(n_way, k_shot):\n    return torch.arange(n_way).repeat_interleave(k_shot).long()\n\n\n# Try to create labels for 5-way 2-shot setting\ncreate_label(5, 2)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-03T07:53:14.346827Z","iopub.execute_input":"2025-07-03T07:53:14.347435Z","iopub.status.idle":"2025-07-03T07:53:14.378377Z","shell.execute_reply.started":"2025-07-03T07:53:14.347406Z","shell.execute_reply":"2025-07-03T07:53:14.377796Z"}},"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"tensor([0, 0, 1, 1, 2, 2, 3, 3, 4, 4])"},"metadata":{}}],"execution_count":7},{"cell_type":"code","source":"def calculate_accuracy(logits, labels):\n    \"\"\"utility function for accuracy calculation\"\"\"\n    acc = np.asarray(\n        [(torch.argmax(logits, -1).cpu().numpy() == labels.cpu().numpy())]\n    ).mean()\n    return acc","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-03T07:53:16.001736Z","iopub.execute_input":"2025-07-03T07:53:16.002446Z","iopub.status.idle":"2025-07-03T07:53:16.006554Z","shell.execute_reply.started":"2025-07-03T07:53:16.002418Z","shell.execute_reply":"2025-07-03T07:53:16.005861Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"# Dataset for train and val\nclass Omniglot(Dataset):\n    def __init__(self, data_dir, k_shot, q_query, task_num=None):\n        self.file_list = [\n            f for f in glob.glob(data_dir + \"**/character*\", recursive=True)\n        ]\n        # limit task number if task_num is set\n        if task_num is not None:\n            self.file_list = self.file_list[: min(len(self.file_list), task_num)]\n        self.transform = transforms.Compose([transforms.ToTensor()])\n        self.n = k_shot + q_query\n\n    def __getitem__(self, idx):\n        # For random sampling the characters we want.\n        img_path = self.file_list[idx]\n        img_list = [f for f in glob.glob(img_path + \"**/*.png\", recursive=True)]\n        img_list.sort()\n        \n        sample = np.arange(len(img_list))\n        np.random.shuffle(sample)\n        \n        # `k_shot + q_query` examples for each character\n        imgs = [self.transform(Image.open(img_list[idx])) for idx in sample[:self.n]]\n        imgs = torch.stack(imgs)\n        return imgs\n\n    def __len__(self):\n        return len(self.file_list)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-03T07:53:27.449503Z","iopub.execute_input":"2025-07-03T07:53:27.449961Z","iopub.status.idle":"2025-07-03T07:53:27.455915Z","shell.execute_reply.started":"2025-07-03T07:53:27.449936Z","shell.execute_reply":"2025-07-03T07:53:27.455293Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"def BaseSolver(\n    model,\n    optimizer,\n    x,\n    n_way,\n    k_shot,\n    q_query,\n    loss_fn,\n    inner_train_step=1,\n    inner_lr=0.4,\n    train=True,\n    return_labels=False,\n):\n    criterion, task_loss, task_acc = loss_fn, [], []\n    labels = []\n\n    for meta_batch in x:\n        # Get data\n        support_set = meta_batch[: n_way * k_shot]\n        query_set = meta_batch[n_way * k_shot :]\n\n        if train:\n            \"\"\" training loop \"\"\"\n            # Use the support set to calculate loss\n            labels = create_label(n_way, k_shot).to(device)\n            logits = model.forward(support_set)\n            loss = criterion(logits, labels)\n\n            task_loss.append(loss)\n            task_acc.append(calculate_accuracy(logits, labels))\n        else:\n            \"\"\" validation / testing loop \"\"\"\n            # First update model with support set images for `inner_train_step` steps\n            fast_weights = OrderedDict(model.named_parameters())\n\n\n            for inner_step in range(inner_train_step):\n                # Simply training\n                train_label = create_label(n_way, k_shot).to(device)\n                logits = model.functional_forward(support_set, fast_weights)\n                loss = criterion(logits, train_label)\n\n                grads = torch.autograd.grad(loss, fast_weights.values(), create_graph=True)\n                # Perform SGD\n                fast_weights = OrderedDict(\n                    (name, param - inner_lr * grad)\n                    for ((name, param), grad) in zip(fast_weights.items(), grads)\n                )\n\n            if not return_labels:\n                \"\"\" validation \"\"\"\n                val_label = create_label(n_way, q_query).to(device)\n\n                logits = model.functional_forward(query_set, fast_weights)\n                loss = criterion(logits, val_label)\n                task_loss.append(loss)\n                task_acc.append(calculate_accuracy(logits, val_label))\n            else:\n                \"\"\" testing \"\"\"\n                logits = model.functional_forward(query_set, fast_weights)\n                labels.extend(torch.argmax(logits, -1).cpu().numpy())\n\n    if return_labels:\n        return labels\n\n    batch_loss = torch.stack(task_loss).mean()\n    task_acc = np.mean(task_acc)\n\n    if train:\n        # Update model\n        model.train()\n        optimizer.zero_grad()\n        batch_loss.backward()\n        optimizer.step()\n\n    return batch_loss, task_acc","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-03T07:53:41.287303Z","iopub.execute_input":"2025-07-03T07:53:41.288122Z","iopub.status.idle":"2025-07-03T07:53:41.296694Z","shell.execute_reply.started":"2025-07-03T07:53:41.288093Z","shell.execute_reply":"2025-07-03T07:53:41.296039Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"def MetaSolver(\n    model,\n    optimizer,\n    x,\n    n_way,\n    k_shot,\n    q_query,\n    loss_fn,\n    inner_train_step=1,\n    inner_lr=0.4,\n    train=True,\n    return_labels=False\n):\n    criterion, task_loss, task_acc = loss_fn, [], []\n    labels = []\n\n    for meta_batch in x:\n        # Get data\n        support_set = meta_batch[: n_way * k_shot]\n        query_set = meta_batch[n_way * k_shot :]\n\n        # Copy the params for inner loop\n        fast_weights = OrderedDict(model.named_parameters())\n\n        ### ---------- INNER TRAIN LOOP ---------- ###\n        for inner_step in range(inner_train_step):\n            # Simply training\n            train_label = create_label(n_way, k_shot).to(device)\n            logits = model.functional_forward(support_set, fast_weights)\n            loss = criterion(logits, train_label)\n            # Inner gradients update! vvvvvvvvvvvvvvvvvvvv #\n            \"\"\" Inner Loop Update \"\"\"\n            # TODO: Finish the inner loop update rule\n            raise NotImplementedError\n            # ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ #\n\n        ### ---------- INNER VALID LOOP ---------- ###\n        if not return_labels:\n            \"\"\" training / validation \"\"\"\n            val_label = create_label(n_way, q_query).to(device)\n\n            # Collect gradients for outer loop\n            logits = model.functional_forward(query_set, fast_weights)\n            loss = criterion(logits, val_label)\n            task_loss.append(loss)\n            task_acc.append(calculate_accuracy(logits, val_label))\n        else:\n            \"\"\" testing \"\"\"\n            logits = model.functional_forward(query_set, fast_weights)\n            labels.extend(torch.argmax(logits, -1).cpu().numpy())\n\n    if return_labels:\n        return labels\n\n    # Update outer loop\n    model.train()\n    optimizer.zero_grad()\n\n    meta_batch_loss = torch.stack(task_loss).mean()\n    if train:\n        \"\"\" Outer Loop Update \"\"\"\n        # TODO: Finish the outer loop update\n        raise NotimplementedError\n\n    task_acc = np.mean(task_acc)\n    return meta_batch_loss, task_acc","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-03T07:53:49.495025Z","iopub.execute_input":"2025-07-03T07:53:49.495706Z","iopub.status.idle":"2025-07-03T07:53:49.503659Z","shell.execute_reply.started":"2025-07-03T07:53:49.495680Z","shell.execute_reply":"2025-07-03T07:53:49.503098Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"n_way = 5\nk_shot = 1\nq_query = 1\ntrain_inner_train_step = 1\nval_inner_train_step = 3\ninner_lr = 0.4\nmeta_lr = 0.001\nmeta_batch_size = 32\nmax_epoch = 30\neval_batches = 20\ntrain_data_path = \"/kaggle/working/Omniglot/images_background\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-03T07:54:17.296546Z","iopub.execute_input":"2025-07-03T07:54:17.297015Z","iopub.status.idle":"2025-07-03T07:54:17.301173Z","shell.execute_reply.started":"2025-07-03T07:54:17.296990Z","shell.execute_reply":"2025-07-03T07:54:17.300479Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"def dataloader_init(datasets, shuffle=True, num_workers=2):\n    train_set, val_set = datasets\n    train_loader = DataLoader(\n        train_set,\n        # The \"batch_size\" here is not \\\n        #    the meta batch size, but  \\\n        #    how many different        \\\n        #    characters in a task,     \\\n        #    i.e. the \"n_way\" in       \\\n        #    few-shot classification.\n        batch_size=n_way,\n        num_workers=num_workers,\n        shuffle=shuffle,\n        drop_last=True,\n    )\n    val_loader = DataLoader(\n        val_set, batch_size=n_way, num_workers=num_workers, shuffle=shuffle, drop_last=True\n    )\n\n    train_iter = iter(train_loader)\n    val_iter = iter(val_loader)\n    return (train_loader, val_loader), (train_iter, val_iter)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-03T07:54:28.993357Z","iopub.execute_input":"2025-07-03T07:54:28.993630Z","iopub.status.idle":"2025-07-03T07:54:28.998706Z","shell.execute_reply.started":"2025-07-03T07:54:28.993611Z","shell.execute_reply":"2025-07-03T07:54:28.997863Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"def model_init():\n    meta_model = Classifier(1, n_way).to(device)\n    optimizer = torch.optim.Adam(meta_model.parameters(), lr=meta_lr)\n    loss_fn = nn.CrossEntropyLoss().to(device)\n    return meta_model, optimizer, loss_fn","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-03T07:54:34.770325Z","iopub.execute_input":"2025-07-03T07:54:34.770598Z","iopub.status.idle":"2025-07-03T07:54:34.774955Z","shell.execute_reply.started":"2025-07-03T07:54:34.770577Z","shell.execute_reply":"2025-07-03T07:54:34.774171Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"def get_meta_batch(meta_batch_size, k_shot, q_query, data_loader, iterator):\n    data = []\n    for _ in range(meta_batch_size):\n        try:\n            # a \"task_data\" tensor is representing \\\n            #     the data of a task, with size of \\\n            #     [n_way, k_shot+q_query, 1, 28, 28]\n            task_data = next(iterator)\n        except StopIteration:\n            iterator = iter(data_loader)\n            task_data = next(iterator)\n        train_data = task_data[:, :k_shot].reshape(-1, 1, 28, 28)\n        val_data = task_data[:, k_shot:].reshape(-1, 1, 28, 28)\n        task_data = torch.cat((train_data, val_data), 0)\n        data.append(task_data)\n    return torch.stack(data).to(device), iterator","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-03T07:54:39.261252Z","iopub.execute_input":"2025-07-03T07:54:39.261921Z","iopub.status.idle":"2025-07-03T07:54:39.266633Z","shell.execute_reply.started":"2025-07-03T07:54:39.261899Z","shell.execute_reply":"2025-07-03T07:54:39.266008Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"# 设置求解器类型（基础版或元学习版）\nsolver = 'base'  # 可选 'base' 或 'meta'\nmeta_model, optimizer, loss_fn = model_init()\n\n# 根据求解器类型初始化数据集和加载器\nif solver == 'base':\n    max_epoch = 5  # 基础求解器只需要5个epoch\n    Solver = BaseSolver\n    \n    # 创建数据集并检查实际长度\n    dataset = Omniglot(train_data_path, k_shot, q_query, task_num=10)\n    actual_length = len(dataset)\n    print(f\"数据集实际长度: {actual_length}\")  # 调试信息\n    \n    # 确保训练集和验证集分割正确\n    if actual_length >= 10:  # 如果数据集足够大\n        train_size = 8  # 训练集大小（可调整）\n        val_size = 2    # 验证集大小\n    else:  # 如果数据集比预期小\n        train_size = int(0.8 * actual_length)  # 80%训练\n        val_size = actual_length - train_size  # 20%验证\n    \n    # 随机分割数据集\n    train_set, val_set = torch.utils.data.random_split(dataset, [train_size, val_size])\n    \n    # 初始化数据加载器\n    (train_loader, val_loader), (train_iter, val_iter) = dataloader_init(\n        (train_set, val_set), \n        shuffle=False  # 不打乱顺序\n    )\n\nelif solver == 'meta':\n    Solver = MetaSolver\n    dataset = Omniglot(train_data_path, k_shot, q_query)\n    train_split = int(0.8 * len(dataset))  # 80%训练\n    val_split = len(dataset) - train_split  # 20%验证\n    train_set, val_set = torch.utils.data.random_split(dataset, [train_split, val_split])\n    (train_loader, val_loader), (train_iter, val_iter) = dataloader_init((train_set, val_set))\nelse:\n    raise NotImplementedError(\"不支持的求解器类型\")\n\n# 主训练循环\nfor epoch in range(max_epoch):\n    print(f\"\\n第 {epoch + 1} 轮训练\")\n    train_meta_loss = []\n    train_acc = []\n    \n    # 使用tqdm显示进度条\n    for step in tqdm(range(max(1, len(train_loader) // meta_batch_size)), desc=\"训练进度\"):\n        try:\n            # 获取一个元批量的数据\n            x, train_iter = get_meta_batch(\n                meta_batch_size, \n                k_shot, \n                q_query, \n                train_loader, \n                train_iter\n            )\n            \n            # 使用求解器训练\n            meta_loss, acc = Solver(\n                meta_model,\n                optimizer,\n                x,\n                n_way,\n                k_shot,\n                q_query,\n                loss_fn, \n                inner_train_step=train_inner_train_step\n            )\n            \n            train_meta_loss.append(meta_loss.item())\n            train_acc.append(acc)\n            \n        except StopIteration:\n            print(\"\\n警告: 数据迭代器已耗尽，重新初始化...\")\n            train_iter = iter(train_loader)\n            continue\n    \n    # 打印训练结果\n    print(\"  平均损失: %.3f\" % np.mean(train_meta_loss), end=\"\\t\")\n    print(\"  准确率: %.2f%%\" % (np.mean(train_acc) * 100))\n\n    # 验证阶段\n    val_acc = []\n    for eval_step in tqdm(range(max(1, len(val_loader) // eval_batches)), desc=\"验证进度\"):\n        try:\n            x, val_iter = get_meta_batch(\n                eval_batches, \n                k_shot, \n                q_query, \n                val_loader, \n                val_iter\n            )\n            _, acc = Solver(\n                meta_model,\n                optimizer,\n                x,\n                n_way,\n                k_shot,\n                q_query,\n                loss_fn,\n                inner_train_step=val_inner_train_step,\n                train=False,  # 验证模式\n            )\n            val_acc.append(acc)\n            \n        except StopIteration:\n            print(\"\\n警告: 验证数据迭代器已耗尽，重新初始化...\")\n            val_iter = iter(val_loader)\n            continue\n    \n    print(\"  验证准确率: %.2f%%\" % (np.mean(val_acc) * 100))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-03T07:59:31.073600Z","iopub.execute_input":"2025-07-03T07:59:31.074407Z","iopub.status.idle":"2025-07-03T07:59:32.142714Z","shell.execute_reply.started":"2025-07-03T07:59:31.074371Z","shell.execute_reply":"2025-07-03T07:59:32.141669Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"name":"stdout","text":"数据集实际长度: 0\n\n第 1 轮训练\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"训练进度:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a909928011c34cc9a640ab1add9caffd"}},"metadata":{}},{"name":"stdout","text":"\n警告: 数据迭代器已耗尽，重新初始化...\n  平均损失: nan\t  准确率: nan%\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/numpy/core/fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.\n  return _methods._mean(a, axis=axis, dtype=dtype,\n/usr/local/lib/python3.11/dist-packages/numpy/core/_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n  ret = ret.dtype.type(ret / rcount)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"验证进度:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"274ea000d559463ba8512d3301c55f26"}},"metadata":{}},{"name":"stdout","text":"\n警告: 验证数据迭代器已耗尽，重新初始化...\n  验证准确率: nan%\n\n第 2 轮训练\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"训练进度:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6df2c766b53a46d0a3ddade1ed0c915a"}},"metadata":{}},{"name":"stdout","text":"\n警告: 数据迭代器已耗尽，重新初始化...\n  平均损失: nan\t  准确率: nan%\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"验证进度:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d38c3964e42e4691933c0f71f3cc2742"}},"metadata":{}},{"name":"stdout","text":"\n警告: 验证数据迭代器已耗尽，重新初始化...\n  验证准确率: nan%\n\n第 3 轮训练\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"训练进度:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d3d47e1cc5834ca5bf6f3daefd981a41"}},"metadata":{}},{"name":"stdout","text":"\n警告: 数据迭代器已耗尽，重新初始化...\n  平均损失: nan\t  准确率: nan%\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"验证进度:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ac6ad849042f40238e554373b69c20f5"}},"metadata":{}},{"name":"stdout","text":"\n警告: 验证数据迭代器已耗尽，重新初始化...\n  验证准确率: nan%\n\n第 4 轮训练\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"训练进度:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bed8a139aa6340c88861cb5bd203959a"}},"metadata":{}},{"name":"stdout","text":"\n警告: 数据迭代器已耗尽，重新初始化...\n  平均损失: nan\t  准确率: nan%\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"验证进度:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8376fabb178a4947bd3499dac084ff06"}},"metadata":{}},{"name":"stdout","text":"\n警告: 验证数据迭代器已耗尽，重新初始化...\n  验证准确率: nan%\n\n第 5 轮训练\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"训练进度:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"74bde6f261114815b5d94109baf559e8"}},"metadata":{}},{"name":"stdout","text":"\n警告: 数据迭代器已耗尽，重新初始化...\n  平均损失: nan\t  准确率: nan%\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"验证进度:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"94a28f8c94b447e381c65e791481d9e2"}},"metadata":{}},{"name":"stdout","text":"\n警告: 验证数据迭代器已耗尽，重新初始化...\n  验证准确率: nan%\n","output_type":"stream"}],"execution_count":21},{"cell_type":"code","source":"import os\n\n# test dataset\nclass OmniglotTest(Dataset):\n    def __init__(self, test_dir):\n        self.test_dir = test_dir\n        self.n = 5\n\n        self.transform = transforms.Compose([transforms.ToTensor()])\n\n    def __getitem__(self, idx):\n        support_files = [\n            os.path.join(self.test_dir, \"support\", f\"{idx:>04}\", f\"image_{i}.png\")\n            for i in range(self.n)\n        ]\n        query_files = [\n            os.path.join(self.test_dir, \"query\", f\"{idx:>04}\", f\"image_{i}.png\")\n            for i in range(self.n)\n        ]\n\n        support_imgs = torch.stack(\n            [self.transform(Image.open(e)) for e in support_files]\n        )\n        query_imgs = torch.stack([self.transform(Image.open(e)) for e in query_files])\n\n        return support_imgs, query_imgs\n\n    def __len__(self):\n        return len(os.listdir(os.path.join(self.test_dir, \"support\")))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-03T07:59:58.290035Z","iopub.execute_input":"2025-07-03T07:59:58.290321Z","iopub.status.idle":"2025-07-03T07:59:58.296359Z","shell.execute_reply.started":"2025-07-03T07:59:58.290299Z","shell.execute_reply":"2025-07-03T07:59:58.295599Z"}},"outputs":[],"execution_count":23},{"cell_type":"code","source":"test_inner_train_step = 10 # you can change this\n\ntest_batches = 20\ntest_dataset = OmniglotTest(\"Omniglot-test\")\ntest_loader = DataLoader(test_dataset, batch_size=test_batches, shuffle=False)\n\noutput = []\nfor _, batch in enumerate(tqdm(test_loader)):\n    support_set, query_set = batch\n    x = torch.cat([support_set, query_set], dim=1)\n    x = x.to(device)\n\n    labels = Solver(\n        meta_model,\n        optimizer,\n        x,\n        n_way,\n        k_shot,\n        q_query,\n        loss_fn,\n        inner_train_step=test_inner_train_step,\n        train=False,\n        return_labels=True,\n    )\n\n    output.extend(labels)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-03T08:00:03.705625Z","iopub.execute_input":"2025-07-03T08:00:03.706105Z","iopub.status.idle":"2025-07-03T08:00:25.905606Z","shell.execute_reply.started":"2025-07-03T08:00:03.706082Z","shell.execute_reply":"2025-07-03T08:00:25.904960Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/32 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"95ccab2e560f43aeb5b82f5f189f61a9"}},"metadata":{}}],"execution_count":24},{"cell_type":"code","source":"# write to csv\nwith open(\"output.csv\", \"w\") as f:\n    f.write(f\"id,class\\n\")\n    for i, label in enumerate(output):\n        f.write(f\"{i},{label}\\n\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-03T08:00:28.075963Z","iopub.execute_input":"2025-07-03T08:00:28.076274Z","iopub.status.idle":"2025-07-03T08:00:28.082797Z","shell.execute_reply.started":"2025-07-03T08:00:28.076252Z","shell.execute_reply":"2025-07-03T08:00:28.082182Z"}},"outputs":[],"execution_count":25}]}