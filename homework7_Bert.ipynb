{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2025-06-28T05:43:03.625867Z",
     "iopub.status.busy": "2025-06-28T05:43:03.625540Z",
     "iopub.status.idle": "2025-06-28T05:43:10.712285Z",
     "shell.execute_reply": "2025-06-28T05:43:10.711301Z",
     "shell.execute_reply.started": "2025-06-28T05:43:03.625848Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/gdown/__main__.py:140: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n",
      "  warnings.warn(\n",
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1TjoBdNlGBhP_J9C66MOY7ILIrydm7ZCS\n",
      "To: /kaggle/working/hw7_data.zip\n",
      "100%|██████████████████████████████████████| 12.1M/12.1M [00:00<00:00, 48.7MB/s]\n"
     ]
    }
   ],
   "source": [
    "!gdown --id '1TjoBdNlGBhP_J9C66MOY7ILIrydm7ZCS' --output hw7_data.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2025-06-28T05:43:18.089615Z",
     "iopub.status.busy": "2025-06-28T05:43:18.088995Z",
     "iopub.status.idle": "2025-06-28T05:43:18.498834Z",
     "shell.execute_reply": "2025-06-28T05:43:18.498121Z",
     "shell.execute_reply.started": "2025-06-28T05:43:18.089578Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  hw7_data.zip\n",
      "  inflating: hw7_train.json          \n",
      "  inflating: hw7_test.json           \n",
      "  inflating: hw7_dev.json            \n",
      "  inflating: hw7_in-context-learning-examples.json  \n"
     ]
    }
   ],
   "source": [
    "!unzip -o hw7_data.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2025-06-28T05:48:55.061029Z",
     "iopub.status.busy": "2025-06-28T05:48:55.060476Z",
     "iopub.status.idle": "2025-06-28T05:49:06.639713Z",
     "shell.execute_reply": "2025-06-28T05:49:06.638801Z",
     "shell.execute_reply.started": "2025-06-28T05:48:55.060988Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: transformers 4.26.1\n",
      "Uninstalling transformers-4.26.1:\n",
      "  Successfully uninstalled transformers-4.26.1\n",
      "Found existing installation: accelerate 0.16.0\n",
      "Uninstalling accelerate-0.16.0:\n",
      "  Successfully uninstalled accelerate-0.16.0\n",
      "Found existing installation: tokenizers 0.13.3\n",
      "Uninstalling tokenizers-0.13.3:\n",
      "  Successfully uninstalled tokenizers-0.13.3\n",
      "Collecting transformers==4.41.0\n",
      "  Downloading transformers-4.41.0-py3-none-any.whl.metadata (43 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.8/43.8 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting accelerate==0.21.0\n",
      "  Downloading accelerate-0.21.0-py3-none-any.whl.metadata (17 kB)\n",
      "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.11/dist-packages (3.4.1)\n",
      "Requirement already satisfied: kaggle-environments in /usr/local/lib/python3.11/dist-packages (1.16.11)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers==4.41.0) (3.18.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from transformers==4.41.0) (0.31.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.41.0) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers==4.41.0) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers==4.41.0) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.41.0) (2024.11.6)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers==4.41.0) (2.32.3)\n",
      "Collecting tokenizers<0.20,>=0.19 (from transformers==4.41.0)\n",
      "  Downloading tokenizers-0.19.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers==4.41.0) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers==4.41.0) (4.67.1)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate==0.21.0) (7.0.0)\n",
      "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.11/dist-packages (from accelerate==0.21.0) (2.6.0+cu124)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.2.2)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.15.2)\n",
      "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (11.1.0)\n",
      "Requirement already satisfied: Chessnut>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from kaggle-environments) (0.4.1)\n",
      "Requirement already satisfied: Flask>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from kaggle-environments) (3.1.0)\n",
      "Requirement already satisfied: gymnasium==0.29.0 in /usr/local/lib/python3.11/dist-packages (from kaggle-environments) (0.29.0)\n",
      "Requirement already satisfied: jsonschema>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from kaggle-environments) (4.23.0)\n",
      "Requirement already satisfied: pettingzoo==1.24.0 in /usr/local/lib/python3.11/dist-packages (from kaggle-environments) (1.24.0)\n",
      "Requirement already satisfied: shimmy>=1.2.1 in /usr/local/lib/python3.11/dist-packages (from kaggle-environments) (1.3.0)\n",
      "Requirement already satisfied: stable-baselines3==2.1.0 in /usr/local/lib/python3.11/dist-packages (from kaggle-environments) (2.1.0)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium==0.29.0->kaggle-environments) (3.1.1)\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium==0.29.0->kaggle-environments) (4.13.2)\n",
      "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from gymnasium==0.29.0->kaggle-environments) (0.0.4)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from stable-baselines3==2.1.0->kaggle-environments) (2.2.3)\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from stable-baselines3==2.1.0->kaggle-environments) (3.7.2)\n",
      "Requirement already satisfied: Werkzeug>=3.1 in /usr/local/lib/python3.11/dist-packages (from Flask>=1.1.2->kaggle-environments) (3.1.3)\n",
      "Requirement already satisfied: Jinja2>=3.1.2 in /usr/local/lib/python3.11/dist-packages (from Flask>=1.1.2->kaggle-environments) (3.1.6)\n",
      "Requirement already satisfied: itsdangerous>=2.2 in /usr/local/lib/python3.11/dist-packages (from Flask>=1.1.2->kaggle-environments) (2.2.0)\n",
      "Requirement already satisfied: click>=8.1.3 in /usr/local/lib/python3.11/dist-packages (from Flask>=1.1.2->kaggle-environments) (8.1.8)\n",
      "Requirement already satisfied: blinker>=1.9 in /usr/local/lib/python3.11/dist-packages (from Flask>=1.1.2->kaggle-environments) (1.9.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.23.0->transformers==4.41.0) (2025.3.2)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.23.0->transformers==4.41.0) (1.1.0)\n",
      "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0.1->kaggle-environments) (25.3.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0.1->kaggle-environments) (2024.10.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0.1->kaggle-environments) (0.36.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0.1->kaggle-environments) (0.24.0)\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers==4.41.0) (1.3.8)\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers==4.41.0) (1.2.4)\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers==4.41.0) (0.1.1)\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers==4.41.0) (2025.1.0)\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers==4.41.0) (2022.1.0)\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers==4.41.0) (2.4.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.41.0) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.41.0) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.41.0) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.41.0) (2025.4.26)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate==0.21.0) (3.4.2)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate==0.21.0) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate==0.21.0) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate==0.21.0) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate==0.21.0) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate==0.21.0) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate==0.21.0) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate==0.21.0) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate==0.21.0) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate==0.21.0) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate==0.21.0) (0.6.2)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate==0.21.0) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate==0.21.0) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate==0.21.0) (12.4.127)\n",
      "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate==0.21.0) (3.2.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate==0.21.0) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.10.0->accelerate==0.21.0) (1.3.0)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (1.5.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from Jinja2>=3.1.2->Flask>=1.1.2->kaggle-environments) (3.0.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->stable-baselines3==2.1.0->kaggle-environments) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->stable-baselines3==2.1.0->kaggle-environments) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->stable-baselines3==2.1.0->kaggle-environments) (4.57.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->stable-baselines3==2.1.0->kaggle-environments) (1.4.8)\n",
      "Requirement already satisfied: pyparsing<3.1,>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->stable-baselines3==2.1.0->kaggle-environments) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib->stable-baselines3==2.1.0->kaggle-environments) (2.9.0.post0)\n",
      "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers==4.41.0) (2024.2.0)\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers==4.41.0) (2022.1.0)\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->transformers==4.41.0) (1.3.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->transformers==4.41.0) (2024.2.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->stable-baselines3==2.1.0->kaggle-environments) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->stable-baselines3==2.1.0->kaggle-environments) (2025.2)\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->transformers==4.41.0) (2024.2.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib->stable-baselines3==2.1.0->kaggle-environments) (1.17.0)\n",
      "Downloading transformers-4.41.0-py3-none-any.whl (9.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.1/9.1 MB\u001b[0m \u001b[31m69.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hDownloading accelerate-0.21.0-py3-none-any.whl (244 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m244.2/244.2 kB\u001b[0m \u001b[31m19.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tokenizers-0.19.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m96.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: tokenizers, transformers, accelerate\n",
      "Successfully installed accelerate-0.21.0 tokenizers-0.19.1 transformers-4.41.0\n"
     ]
    }
   ],
   "source": [
    "# 先卸载冲突版本\n",
    "!pip uninstall transformers accelerate tokenizers -y\n",
    "\n",
    "# 安装兼容版本\n",
    "!pip install transformers==4.41.0 accelerate==0.21.0 sentence-transformers kaggle-environments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-28T05:50:34.543057Z",
     "iopub.status.busy": "2025-06-28T05:50:34.542624Z",
     "iopub.status.idle": "2025-06-28T05:50:34.550400Z",
     "shell.execute_reply": "2025-06-28T05:50:34.549714Z",
     "shell.execute_reply.started": "2025-06-28T05:50:34.542997Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import complete!\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset \n",
    "from transformers import AdamW\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Fix random seed for reproducibility\n",
    "def same_seeds(seed):\n",
    "\ttorch.manual_seed(seed)\n",
    "\tif torch.cuda.is_available():\n",
    "\t\t\ttorch.cuda.manual_seed(seed)\n",
    "\t\t\ttorch.cuda.manual_seed_all(seed)\n",
    "\tnp.random.seed(seed)\n",
    "\trandom.seed(seed)\n",
    "\ttorch.backends.cudnn.benchmark = False\n",
    "\ttorch.backends.cudnn.deterministic = True\n",
    "same_seeds(2)\n",
    "\n",
    "print(\"import complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-28T06:06:08.846300Z",
     "iopub.status.busy": "2025-06-28T06:06:08.845510Z",
     "iopub.status.idle": "2025-06-28T06:06:12.856182Z",
     "shell.execute_reply": "2025-06-28T06:06:12.855383Z",
     "shell.execute_reply.started": "2025-06-28T06:06:08.846267Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec5bede4dcaf4ea7a52c976fefb7c1fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/624 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10bbc2da937f47c1b5aa477b7776aaec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/412M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-base-chinese and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4218cdb078124fedb9597057abe64d01",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/49.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "565e30b858cc4274bbf7c3aa866bd091",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/110k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d63ad5e61f9e417fb8b2601de170b29e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/269k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import complete!\n"
     ]
    }
   ],
   "source": [
    "from transformers import (\n",
    "  AutoTokenizer,\n",
    "  AutoModelForQuestionAnswering,\n",
    ")\n",
    "\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(\"bert-base-chinese\").to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-chinese\")\n",
    "\n",
    "print(\"import complete!\")\n",
    "\n",
    "# You can safely ignore the warning message (it pops up because new prediction heads for QA are initialized randomly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-28T06:06:32.683079Z",
     "iopub.status.busy": "2025-06-28T06:06:32.682757Z",
     "iopub.status.idle": "2025-06-28T06:06:32.932587Z",
     "shell.execute_reply": "2025-06-28T06:06:32.931805Z",
     "shell.execute_reply.started": "2025-06-28T06:06:32.683056Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "function defined!\n"
     ]
    }
   ],
   "source": [
    "def read_data(file):\n",
    "    with open(file, 'r', encoding=\"utf-8\") as reader:\n",
    "        data = json.load(reader)\n",
    "    return data[\"questions\"], data[\"paragraphs\"]\n",
    "\n",
    "train_questions, train_paragraphs = read_data(\"hw7_train.json\")\n",
    "dev_questions, dev_paragraphs = read_data(\"hw7_dev.json\")\n",
    "test_questions, test_paragraphs = read_data(\"hw7_test.json\")\n",
    "\n",
    "print(\"function defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-28T06:06:49.498194Z",
     "iopub.status.busy": "2025-06-28T06:06:49.497675Z",
     "iopub.status.idle": "2025-06-28T06:06:58.106259Z",
     "shell.execute_reply": "2025-06-28T06:06:58.105518Z",
     "shell.execute_reply.started": "2025-06-28T06:06:49.498169Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "complete!\n"
     ]
    }
   ],
   "source": [
    "# Tokenize questions and paragraphs separately\n",
    "# 「add_special_tokens」 is set to False since special tokens will be added when tokenized questions and paragraphs are combined in datset __getitem__ \n",
    "\n",
    "train_questions_tokenized = tokenizer([train_question[\"question_text\"] for train_question in train_questions], add_special_tokens=False)\n",
    "dev_questions_tokenized = tokenizer([dev_question[\"question_text\"] for dev_question in dev_questions], add_special_tokens=False)\n",
    "test_questions_tokenized = tokenizer([test_question[\"question_text\"] for test_question in test_questions], add_special_tokens=False) \n",
    "\n",
    "train_paragraphs_tokenized = tokenizer(train_paragraphs, add_special_tokens=False)\n",
    "dev_paragraphs_tokenized = tokenizer(dev_paragraphs, add_special_tokens=False)\n",
    "test_paragraphs_tokenized = tokenizer(test_paragraphs, add_special_tokens=False)\n",
    "\n",
    "print(\"complete!\")\n",
    "\n",
    "# You can safely ignore the warning message as tokenized sequences will be futher processed in datset __getitem__ before passing to model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-28T07:11:01.233783Z",
     "iopub.status.busy": "2025-06-28T07:11:01.233514Z",
     "iopub.status.idle": "2025-06-28T07:11:01.247181Z",
     "shell.execute_reply": "2025-06-28T07:11:01.246579Z",
     "shell.execute_reply.started": "2025-06-28T07:11:01.233763Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class defined!\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "class QA_Dataset(Dataset):\n",
    "    def __init__(self, split, questions, tokenized_questions, tokenized_paragraphs):\n",
    "        self.split = split\n",
    "        self.questions = questions\n",
    "        self.tokenized_questions = tokenized_questions\n",
    "        self.tokenized_paragraphs = tokenized_paragraphs\n",
    "        self.max_question_len = 60\n",
    "        self.max_paragraph_len = 150\n",
    "        self.min_paragraph_len = 50 # 新增最小长度\n",
    "        self.doc_stride = 75 # 减小步长\n",
    "        \n",
    "\n",
    "        # Input sequence length = [CLS] + question + [SEP] + paragraph + [SEP]\n",
    "        self.max_seq_len = 1 + self.max_question_len + 1 + self.max_paragraph_len + 1\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.questions)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        question = self.questions[idx]\n",
    "        tokenized_question = self.tokenized_questions[idx]\n",
    "        tokenized_paragraph = self.tokenized_paragraphs[question[\"paragraph_id\"]]\n",
    "\n",
    "        ##### Preprocessing这块准备大改 #####\n",
    "        # Hint: How to prevent model from learning something it should not learn\n",
    "        if self.split == \"train\":\n",
    "            \n",
    "            # 初始化默认值\n",
    "            answer_start_token = -1\n",
    "            answer_end_token = -1\n",
    "            paragraph_start = 0\n",
    "            \n",
    "            # 10%概率使用无答案样本,减弱计算机幻觉\n",
    "            if random.random() < 0.1:\n",
    "                answer_start_token = answer_end_token = -1\n",
    "                paragraph_start = random.randint(0,max(0,len(tokenized_paragraph) - self.max_paragraph_len))\n",
    "\n",
    "            else:\n",
    "                answer_start_token = tokenized_paragraph.char_to_token(question[\"answer_start\"])\n",
    "                answer_end_token = tokenized_paragraph.char_to_token(question[\"answer_end\"])\n",
    "\n",
    "                # 随即偏移窗口，防止模型思维定式,只知道从中间开始找\n",
    "                offset = random.randint(-self.max_paragraph_len // 4, self.max_paragraph_len // 4)\n",
    "                mid = (answer_start_token + answer_end_token) // 2 + offset\n",
    "                paragraph_start = max(0, min(mid - self.max_paragraph_len // 2, \n",
    "                                     len(tokenized_paragraph) - self.max_paragraph_len))\n",
    "                \n",
    "            # 动态长度\n",
    "            current_paragraph_len = random.randint(self.min_paragraph_len,self.max_paragraph_len)\n",
    "            paragraph_end = min(paragraph_start + current_paragraph_len, len(tokenized_paragraph))\n",
    "            \n",
    "\n",
    "            # Slice question/paragraph and add special tokens (101: CLS, 102: SEP)\n",
    "            input_ids_question = [101] + tokenized_question.ids[:self.max_question_len] + [102] \n",
    "            input_ids_paragraph = tokenized_paragraph.ids[paragraph_start : paragraph_end] + [102]\t\t\n",
    "\n",
    "            if answer_start_token != -1:\n",
    "                answer_start_token += len(input_ids_question) - paragraph_start\n",
    "                answer_end_token += len(input_ids_question) - paragraph_start\n",
    "            else:\n",
    "                answer_start_token = answer_end_token = 0 # 无答案时设置为0\n",
    "                \n",
    "            \n",
    "            # Pad sequence and obtain inputs to model \n",
    "            input_ids, token_type_ids, attention_mask = self.padding(input_ids_question, input_ids_paragraph)\n",
    "            return torch.tensor(input_ids), torch.tensor(token_type_ids), torch.tensor(attention_mask), answer_start_token, answer_end_token\n",
    "\n",
    "        # Validation/Testing\n",
    "        else:\n",
    "            input_ids_list, token_type_ids_list, attention_mask_list = [], [], []\n",
    "            \n",
    "            # Paragraph is split into several windows, each with start positions separated by step \"doc_stride\"\n",
    "            for i in range(0, len(tokenized_paragraph), self.doc_stride):\n",
    "                \n",
    "                # Slice question/paragraph and add special tokens (101: CLS, 102: SEP)\n",
    "                input_ids_question = [101] + tokenized_question.ids[:self.max_question_len] + [102]\n",
    "                input_ids_paragraph = tokenized_paragraph.ids[i : i + self.max_paragraph_len] + [102]\n",
    "                \n",
    "                # Pad sequence and obtain inputs to model\n",
    "                input_ids, token_type_ids, attention_mask = self.padding(input_ids_question, input_ids_paragraph)\n",
    "                \n",
    "                input_ids_list.append(input_ids)\n",
    "                token_type_ids_list.append(token_type_ids)\n",
    "                attention_mask_list.append(attention_mask)\n",
    "            \n",
    "            return torch.tensor(input_ids_list), torch.tensor(token_type_ids_list), torch.tensor(attention_mask_list)\n",
    "\n",
    "    def padding(self, input_ids_question, input_ids_paragraph):\n",
    "        # Pad zeros if sequence length is shorter than max_seq_len\n",
    "        padding_len = self.max_seq_len - len(input_ids_question) - len(input_ids_paragraph)\n",
    "        # Indices of input sequence tokens in the vocabulary\n",
    "        input_ids = input_ids_question + input_ids_paragraph + [0] * padding_len\n",
    "        # Segment token indices to indicate first and second portions of the inputs. Indices are selected in [0, 1]\n",
    "        token_type_ids = [0] * len(input_ids_question) + [1] * len(input_ids_paragraph) + [0] * padding_len\n",
    "        # Mask to avoid performing attention on padding token indices. Mask values selected in [0, 1]\n",
    "        attention_mask = [1] * (len(input_ids_question) + len(input_ids_paragraph)) + [0] * padding_len\n",
    "        \n",
    "        return input_ids, token_type_ids, attention_mask\n",
    "\n",
    "train_set = QA_Dataset(\"train\", train_questions, train_questions_tokenized, train_paragraphs_tokenized)\n",
    "dev_set = QA_Dataset(\"dev\", dev_questions, dev_questions_tokenized, dev_paragraphs_tokenized)\n",
    "test_set = QA_Dataset(\"test\", test_questions, test_questions_tokenized, test_paragraphs_tokenized)\n",
    "\n",
    "print(\"class defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-28T07:11:04.994146Z",
     "iopub.status.busy": "2025-06-28T07:11:04.993840Z",
     "iopub.status.idle": "2025-06-28T07:11:05.000624Z",
     "shell.execute_reply": "2025-06-28T07:11:04.999843Z",
     "shell.execute_reply.started": "2025-06-28T07:11:04.994125Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "function defined!s\n"
     ]
    }
   ],
   "source": [
    "def evaluate(data, output, tokenizer):\n",
    "    ##### TODO: Postprocessing #####\n",
    "    # There is a bug and room for improvement in postprocessing \n",
    "    # Hint: Open your prediction file to see what is wrong \n",
    "    \n",
    "    answer = \"\"\n",
    "    max_prob = float('-inf')\n",
    "    num_of_windows = data[0].shape[1] # 获取窗口数量\n",
    "    \n",
    "    for k in range(num_of_windows):\n",
    "        # 获取当前窗口的start和end logits\n",
    "        start_logits = output.start_logits[k]\n",
    "        end_logits = output.end_logits[k]\n",
    "        \n",
    "        # 找到概率最高的start和end位置\n",
    "        start_prob, start_index = torch.max(output.start_logits[k], dim=0)\n",
    "        end_prob, end_index = torch.max(output.end_logits[k], dim=0)\n",
    "\n",
    "        # 检查是否为无效答案（start > end 或 指向[CLS]/[SEP]）\n",
    "        if start_index > end_index:\n",
    "            continue # 跳过无效窗口\n",
    "        # 计算联合概率（可改用对数概率或其他加权方式）\n",
    "        prob = start_prob + end_prob\n",
    "        \n",
    "       # 更新最佳答案\n",
    "        if prob > max_prob:\n",
    "            max_prob = prob\n",
    "            # 解码token（跳过特殊符号如[CLS]、[SEP]）\n",
    "            tokens = data[0][0][k][start_index : end_index + 1]\n",
    "            answer = tokenizer.decode(tokens, skip_special_tokens = True)\n",
    "\n",
    "    # 后处理: 去除空格,无效符号等\n",
    "    answer = answer.replace(' ','' )\n",
    "    \n",
    "     # 如果所有窗口均无效，返回空字符串\n",
    "    return answer if answer else \"\"\n",
    "\n",
    "print(\"function defined!s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-28T07:12:55.756670Z",
     "iopub.status.busy": "2025-06-28T07:12:55.755788Z",
     "iopub.status.idle": "2025-06-28T07:35:03.321200Z",
     "shell.execute_reply": "2025-06-28T07:35:03.320581Z",
     "shell.execute_reply.started": "2025-06-28T07:12:55.756636Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Training ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  45%|████▍     | 3002/6730 [02:49<03:23, 18.32it/s, loss=0.00351, acc=0.00025, lr=2.78e-6] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1 | Step 3000 | Loss: 4.1369 | Acc: 0.0648\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  89%|████████▉ | 6002/6730 [05:38<00:41, 17.35it/s, loss=0.00106, acc=0.00025, lr=5.56e-6] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1 | Step 6000 | Loss: 2.5821 | Acc: 0.3207\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 6730/6730 [06:19<00:00, 17.74it/s, loss=0.247, acc=0.0442, lr=6.24e-6]    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating Dev Set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2863/2863 [01:03<00:00, 44.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation | Epoch 1 | Acc: 0.6154\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2:  45%|████▍     | 3002/6730 [02:48<03:20, 18.62it/s, loss=0.00196, acc=0.0005, lr=9.02e-6]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2 | Step 3000 | Loss: 2.0787 | Acc: 0.4494\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2:  89%|████████▉ | 6002/6730 [05:37<00:41, 17.34it/s, loss=0.000801, acc=0.000333, lr=1.18e-5]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2 | Step 6000 | Loss: 1.9742 | Acc: 0.4765\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 6730/6730 [06:17<00:00, 17.83it/s, loss=0.215, acc=0.0513, lr=1.25e-5]     \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating Dev Set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2863/2863 [01:03<00:00, 44.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation | Epoch 2 | Acc: 0.7035\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3:  45%|████▍     | 3002/6730 [02:48<03:25, 18.16it/s, loss=0.00227, acc=0.0005, lr=1.53e-5]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 3 | Step 3000 | Loss: 1.8187 | Acc: 0.5192\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3:  89%|████████▉ | 6002/6730 [05:37<00:41, 17.34it/s, loss=0.00134, acc=0.000167, lr=1.81e-5] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 3 | Step 6000 | Loss: 1.7540 | Acc: 0.5306\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 6730/6730 [06:18<00:00, 17.76it/s, loss=0.194, acc=0.0572, lr=1.88e-5]    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating Dev Set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2863/2863 [01:03<00:00, 44.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation | Epoch 3 | Acc: 0.7461\n",
      "\n",
      "Saving Model...\n"
     ]
    }
   ],
   "source": [
    "from accelerate import Accelerator\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "\n",
    "# hyperparameters\n",
    "num_epoch = 3\n",
    "validation = True\n",
    "logging_step = 3000\n",
    "learning_rate = 3e-5\n",
    "train_batch_size = 4 \n",
    "gradient_accumulation_steps = 16\n",
    "warmup_ratio = 0.1  # 学习率warmup比例\n",
    "fp16_training = True\n",
    "# 初始化Accelerator\n",
    "accelerator = Accelerator(\n",
    "    mixed_precision=\"fp16\" if fp16_training else \"no\",\n",
    "    gradient_accumulation_steps=gradient_accumulation_steps\n",
    ")\n",
    "\n",
    "# 数据加载器\n",
    "train_loader = DataLoader(train_set, batch_size=train_batch_size, shuffle=True, pin_memory=True)\n",
    "dev_loader = DataLoader(dev_set, batch_size=1, shuffle=False, pin_memory=True)\n",
    "test_loader = DataLoader(test_set, batch_size=1, shuffle=False, pin_memory=True)\n",
    "\n",
    "# 优化器和学习率调度\n",
    "optimizer = AdamW(model.parameters(), lr=learning_rate, correct_bias=False)\n",
    "total_steps = len(train_loader) * num_epoch\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=int(total_steps * warmup_ratio),\n",
    "    num_training_steps=total_steps\n",
    ")\n",
    "\n",
    "model, optimizer, train_loader, scheduler = accelerator.prepare(\n",
    "    model, optimizer, train_loader, scheduler\n",
    ") \n",
    "\n",
    "\n",
    "#### TODO: gradient_accumulation (optional)####\n",
    "# Note: train_batch_size * gradient_accumulation_steps = effective batch size\n",
    "# If CUDA out of memory, you can make train_batch_size lower and gradient_accumulation_steps upper\n",
    "# Doc: https://huggingface.co/docs/accelerate/usage_guides/gradient_accumulation\n",
    "\n",
    "\n",
    "# Documentation for the toolkit:  https://huggingface.co/docs/accelerate/\n",
    "\n",
    "print(\"Start Training ...\")\n",
    "\n",
    "for epoch in range(num_epoch):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct_predictions = 0\n",
    "\n",
    "    progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch + 1}\")\n",
    "    for step, data in enumerate(progress_bar):\n",
    "        data = [i.to(accelerator.device) for i in data]\n",
    "\n",
    "        with accelerator.accumulate(model):\n",
    "            output = model(\n",
    "                input_ids=data[0],\n",
    "                token_type_ids=data[1],\n",
    "                attention_mask=data[2],\n",
    "                start_positions=data[3],\n",
    "                end_positions=data[4]\n",
    "            )\n",
    "            loss = output.loss\n",
    "            accelerator.backward(loss)\n",
    "            \n",
    "            # 梯度裁剪\n",
    "            if accelerator.sync_gradients:\n",
    "                accelerator.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            \n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # 计算准确率\n",
    "            start_pred = torch.argmax(output.start_logits, dim=1)\n",
    "            end_pred = torch.argmax(output.end_logits, dim=1)\n",
    "            correct = ((start_pred == data[3]) & (end_pred == data[4])).float().mean()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            correct_predictions += correct.item()\n",
    "            \n",
    "            # 更新进度条\n",
    "            progress_bar.set_postfix({\n",
    "                'loss': total_loss / (step + 1),\n",
    "                'acc': correct_predictions / (step + 1),\n",
    "                'lr': scheduler.get_last_lr()[0]\n",
    "            })\n",
    "        \n",
    "        # 定期日志记录\n",
    "        if (step + 1) % logging_step == 0:\n",
    "            avg_loss = total_loss / logging_step\n",
    "            avg_acc = correct_predictions / logging_step\n",
    "            print(f\"\\nEpoch {epoch+1} | Step {step+1} | Loss: {avg_loss:.4f} | Acc: {avg_acc:.4f}\")\n",
    "            total_loss = correct_predictions = 0\n",
    "\n",
    "    # 验证集评估\n",
    "    if validation:\n",
    "        print(\"\\nEvaluating Dev Set...\")\n",
    "        model.eval()\n",
    "        dev_correct = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for i, data in enumerate(tqdm(dev_loader)):\n",
    "                output = model(\n",
    "                    input_ids=data[0].squeeze(0).to(accelerator.device),\n",
    "                    token_type_ids=data[1].squeeze(0).to(accelerator.device),\n",
    "                    attention_mask=data[2].squeeze(0).to(accelerator.device)\n",
    "                )\n",
    "                pred_answer = evaluate(data, output, tokenizer)\n",
    "                dev_correct += int(pred_answer == dev_questions[i][\"answer_text\"])\n",
    "        \n",
    "        dev_acc = dev_correct / len(dev_loader)\n",
    "        print(f\"Validation | Epoch {epoch+1} | Acc: {dev_acc:.4f}\")\n",
    "\n",
    "# 保存模型\n",
    "print(\"\\nSaving Model...\")\n",
    "accelerator.wait_for_everyone()\n",
    "unwrapped_model = accelerator.unwrap_model(model)\n",
    "unwrapped_model.save_pretrained(\"saved_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-28T07:39:44.827047Z",
     "iopub.status.busy": "2025-06-28T07:39:44.826290Z",
     "iopub.status.idle": "2025-06-28T07:41:05.907758Z",
     "shell.execute_reply": "2025-06-28T07:41:05.907046Z",
     "shell.execute_reply.started": "2025-06-28T07:39:44.826995Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Test Set ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3524/3524 [01:21<00:00, 43.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed! Result is in result.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Evaluating Test Set ...\")\n",
    "\n",
    "result = []\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for data in tqdm(test_loader):\n",
    "        output = model(input_ids=data[0].squeeze(dim=0).to(device), token_type_ids=data[1].squeeze(dim=0).to(device),\n",
    "                       attention_mask=data[2].squeeze(dim=0).to(device))\n",
    "        result.append(evaluate(data, output, tokenizer))\n",
    "\n",
    "result_file = \"result.csv\"\n",
    "with open(result_file, 'w') as f:\t\n",
    "    f.write(\"ID,Answer\\n\")\n",
    "    for i, test_question in enumerate(test_questions):\n",
    "    # Replace commas in answers with empty strings (since csv is separated by comma)\n",
    "    # Answers in kaggle are processed in the same way\n",
    "        f.write(f\"{test_question['id']},{result[i].replace(',','')}\\n\")\n",
    "\n",
    "print(f\"Completed! Result is in {result_file}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
