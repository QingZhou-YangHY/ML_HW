{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":12258362,"sourceType":"datasetVersion","datasetId":7724463}],"dockerImageVersionId":31041,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nprint(os.listdir(\"kaggle/input/homework2dataset/libriphone\"))","metadata":{"execution":{"iopub.status.busy":"2025-06-23T14:44:22.676517Z","iopub.execute_input":"2025-06-23T14:44:22.676816Z","iopub.status.idle":"2025-06-23T14:44:22.684937Z","shell.execute_reply.started":"2025-06-23T14:44:22.676793Z","shell.execute_reply":"2025-06-23T14:44:22.684199Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nimport torch\nimport random\nimport os\nimport gc\n\nfrom tqdm import tqdm\nfrom torch.utils.data import Dataset, DataLoader\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.optim.lr_scheduler as lr_scheduler\n\ndef same_seeds(seed):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed(seed)\n        torch.cuda.manual_seed_all(seed)\n    # cudnn.benchmark = False 可能会略微降低性能，但确保每次运行结果一致\n    torch.backends.cudnn.benchmark = False\n    torch.backends.cudnn.deterministic = True\n\ndef load_feat(path):\n\n    feat = torch.load(path)\n    return feat\n\ndef shift(x, n):\n\n    if n < 0:\n        left = x[0].repeat(-n, 1)\n        right = x[:n]\n    elif n > 0:\n        right = x[-1].repeat(n, 1)\n        left = x[n:]\n    else:\n        return x\n    return torch.cat((left, right), dim=0)\n\n\ndef concat_feat(x, concat_n):\n\n    assert concat_n % 2 == 1\n    if concat_n < 2:\n        return x\n\n    seq_len, feature_dim = x.size(0), x.size(1)\n\n    x_padded = x.repeat(1, concat_n).view(seq_len, concat_n, feature_dim).permute(1, 0, 2)\n\n    mid = (concat_n // 2)\n    for r_idx in range(1, mid + 1):\n        x_padded[mid + r_idx, :, :] = shift(x_padded[mid + r_idx, :, :].clone(), r_idx)\n        x_padded[mid - r_idx, :, :] = shift(x_padded[mid - r_idx, :, :].clone(), -r_idx)\n\n    return x_padded.permute(1, 0, 2).reshape(seq_len, concat_n * feature_dim)\n\ndef preprocess_data(split, feat_dir, phone_path, concat_nframes, train_ratio=0.8):\n\n    class_num = 41\n\n    if split == 'train' or split == 'val':\n        mode = 'train'\n    elif split == 'test':\n        mode = 'test'\n    else:\n        raise ValueError('Invalid \\'split\\' argument for dataset: PhoneDataset!')\n\n    label_dict = {}\n    if mode == 'train':\n        with open(os.path.join(phone_path, f'{mode}_labels.txt')) as f:\n            for line in f.readlines():\n                line = line.strip('\\n').split(' ')\n                label_dict[line[0]] = [int(p) for p in line[1:]]\n\n        with open(os.path.join(phone_path, 'train_split.txt')) as f:\n            usage_list = f.readlines()\n        random.shuffle(usage_list)\n        train_len = int(len(usage_list) * train_ratio)\n        usage_list = [line.strip('\\n') for line in usage_list]\n        usage_list = usage_list[:train_len] if split == 'train' else usage_list[train_len:]\n\n    elif mode == 'test':\n        with open(os.path.join(phone_path, 'test_split.txt')) as f:\n            usage_list = f.readlines()\n        usage_list = [line.strip('\\n') for line in usage_list]\n\n    print(f'[Dataset] - # phone classes: {class_num}, number of utterances for {split}: {len(usage_list)}')\n\n    all_feats = []\n    all_labels = [] if mode == 'train' else None\n\n    for fname in tqdm(usage_list, desc=f\"Loading {split} data\"):\n        feat = load_feat(os.path.join(feat_dir, mode, f'{fname}.pt'))\n        feat = concat_feat(feat, concat_nframes)  # 应用局部上下文拼接\n\n        all_feats.append(feat)\n\n        if mode == 'train':\n            label = torch.LongTensor(label_dict[fname])\n            all_labels.append(label)\n\n    print(f'[INFO] {split} set loaded. Total {len(all_feats)} utterances.')\n\n    if mode == 'train':\n        return all_feats, all_labels\n    else:\n        return all_feats","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-23T15:22:10.761512Z","iopub.execute_input":"2025-06-23T15:22:10.761996Z","iopub.status.idle":"2025-06-23T15:22:10.775815Z","shell.execute_reply.started":"2025-06-23T15:22:10.761976Z","shell.execute_reply":"2025-06-23T15:22:10.774967Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class LibriDataset(Dataset):\n\n    def __init__(self, X, y=None):\n        self.data = X\n        self.label = y\n\n    def __getitem__(self, idx):\n        if self.label is not None:\n            return self.data[idx], self.label[idx]\n        else:\n            return self.data[idx]\n\n    def __len__(self):\n        return len(self.data)\n\n\nclass PadSequence:\n\n    def __call__(self, batch):\n\n        has_labels = len(batch[0]) == 2\n\n        if has_labels:\n            features = [item[0] for item in batch]\n            labels = [item[1] for item in batch]\n        else:\n            features = [item for item in batch]\n\n        max_len = max([f.size(0) for f in features])\n\n        padded_features = []\n        for f in features:\n            pad_tensor = torch.zeros(max_len - f.size(0), f.size(1), dtype=f.dtype)\n            padded_features.append(torch.cat([f, pad_tensor], dim=0))\n        padded_features = torch.stack(padded_features)  # (batch_size, max_len, feature_dim)\n\n        if has_labels:\n            # 填充标签序列 (-100 是一个常见的忽略索引，或者可以用0)\n            padded_labels = []\n            for l in labels:\n                pad_tensor = torch.full((max_len - l.size(0),), -100, dtype=l.dtype)\n                padded_labels.append(torch.cat([l, pad_tensor], dim=0))\n            padded_labels = torch.stack(padded_labels)\n            return padded_features, padded_labels\n        else:\n            return padded_features\n\n# --- LSTM 分类器模型 ---\nclass LSTMClassifier(nn.Module):\n\n    def __init__(self, input_feature_dim, hidden_dim, num_layers, output_dim, dropout_p=0.5, bidirectional=True):\n        super(LSTMClassifier, self).__init__()\n\n        self.hidden_dim = hidden_dim\n        self.num_layers = num_layers\n        self.bidirectional = bidirectional\n\n        self.lstm = nn.LSTM(input_feature_dim, hidden_dim, num_layers,\n                            batch_first=True,  # 输入形状是 (batch, seq, feature)\n                            dropout=dropout_p if num_layers > 1 else 0,  # 多层时才应用dropout\n                            bidirectional=bidirectional)\n\n        fc_input_dim = hidden_dim * 2 if bidirectional else hidden_dim\n        self.fc = nn.Linear(fc_input_dim, output_dim)\n\n    def forward(self, x):\n\n        num_directions = 2 if self.bidirectional else 1\n\n        h0 = torch.zeros(self.num_layers * num_directions, x.size(0), self.hidden_dim).to(x.device)\n        c0 = torch.zeros(self.num_layers * num_directions, x.size(0), self.hidden_dim).to(x.device)\n\n        output, (hn, cn) = self.lstm(x, (h0.detach(), c0.detach()))\n        output = self.fc(output.reshape(-1, output.size(2)))\n        return output\n\nconcat_nframes = 5\ntrain_ratio = 0.75\n\nseed = 42\nbatch_size = 32\nnum_epoch = 30\nlearning_rate = 1e-3\nmodel_path = '/kaggle/working/lstm_phone_classifier.ckpt'\n\n\noriginal_feature_dim = 39\ninput_feature_dim_for_lstm = original_feature_dim * concat_nframes\nlstm_hidden_dim = 256\nlstm_num_layers = 3\nlstm_dropout_p = 0.3\nbidirectional = True\n\nclass_num = 41  # 电话（音素）分类的类别数量\n\nsame_seeds(seed)\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\nprint(f'DEVICE: {device}')\nprint(\"--- Preparing Data ---\")\ntrain_X, train_y = preprocess_data(split='train', feat_dir='kaggle/input/homework2dataset/libriphone/feat', phone_path='kaggle/input/homework2dataset/libriphone', concat_nframes=concat_nframes,\n                                   train_ratio=train_ratio)\nval_X, val_y = preprocess_data(split='val', feat_dir='kaggle/input/homework2dataset/libriphone/feat', phone_path='kaggle/input/homework2dataset/libriphone', concat_nframes=concat_nframes,\n                               train_ratio=train_ratio)\n\ntrain_set = LibriDataset(train_X, train_y)\nval_set = LibriDataset(val_X, val_y)\n\n\ndel train_X, train_y, val_X, val_y\ngc.collect()\n\ntrain_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True, collate_fn=PadSequence())\nval_loader = DataLoader(val_set, batch_size=batch_size, shuffle=False, collate_fn=PadSequence())\nprint(\"--- Data Prepared ---\")\n\nmodel = LSTMClassifier(\n    input_feature_dim=input_feature_dim_for_lstm,\n    hidden_dim=lstm_hidden_dim,\n    num_layers=lstm_num_layers,\n    output_dim=class_num,\n    dropout_p=lstm_dropout_p,\n    bidirectional=bidirectional\n).to(device)\n\ncriterion = nn.CrossEntropyLoss(ignore_index=-100)\noptimizer = optim.Adam(model.parameters(), lr=learning_rate)\n\nscheduler = lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=5, verbose=True)\n\nprint(f\"--- Model Initialized: {model.__class__.__name__} ---\")\nprint(model)\n\ndel train_set, val_set, train_loader, val_loader\ngc.collect()\ntorch.cuda.empty_cache()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-23T15:22:15.172615Z","iopub.execute_input":"2025-06-23T15:22:15.173327Z","iopub.status.idle":"2025-06-23T15:22:22.473975Z","shell.execute_reply.started":"2025-06-23T15:22:15.173299Z","shell.execute_reply":"2025-06-23T15:22:22.473119Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"--- Start Training ---\")\nbest_val_acc = 0.0\n\nfor epoch in range(num_epoch):\n    model.train()\n\n    total_train_loss = 0.0\n    correct_train_predictions = 0\n    total_train_effective_labels = 0\n\n\n    for batch_idx, (features, labels) in enumerate(tqdm(train_loader, desc=f\"Epoch {epoch + 1}/{num_epoch} [Train]\")):\n        features = features.to(device)\n        labels = labels.to(device)\n\n        optimizer.zero_grad()\n        outputs = model(features)\n\n        active_labels = labels.view(-1)\n        active_outputs = outputs[active_labels != -100]\n        active_labels = active_labels[active_labels != -100]\n\n        if active_labels.numel() == 0:\n            continue\n\n        loss = criterion(active_outputs, active_labels)\n        loss.backward()\n        optimizer.step()\n\n        _, train_pred = torch.max(active_outputs, 1)\n        correct_train_predictions += (train_pred == active_labels).sum().item()\n        total_train_loss += loss.item() * len(active_labels)\n        total_train_effective_labels += len(active_labels)\n\n    model.eval()\n    total_val_loss = 0.0\n    correct_val_predictions = 0\n    total_val_effective_labels = 0\n\n    with torch.no_grad():\n        for batch_idx, (features, labels) in enumerate(tqdm(val_loader, desc=f\"Epoch {epoch + 1}/{num_epoch} [Val]\")):\n            features = features.to(device)\n            labels = labels.to(device)\n            outputs = model(features)\n\n            active_labels = labels.view(-1)\n            active_outputs = outputs[active_labels != -100]\n            active_labels = active_labels[active_labels != -100]\n\n            if active_labels.numel() == 0:\n                continue\n\n            loss = criterion(active_outputs, active_labels)\n\n            _, val_pred = torch.max(active_outputs, 1)\n            correct_val_predictions += (val_pred == active_labels).sum().item()\n            total_val_loss += loss.item() * len(active_labels)\n            total_val_effective_labels += len(active_labels)\n\n\n    avg_train_acc = correct_train_predictions / total_train_effective_labels if total_train_effective_labels > 0 else 0\n    avg_train_loss = total_train_loss / total_train_effective_labels if total_train_effective_labels > 0 else 0\n    avg_val_acc = correct_val_predictions / total_val_effective_labels if total_val_effective_labels > 0 else 0\n    avg_val_loss = total_val_loss / total_val_effective_labels if total_val_effective_labels > 0 else 0\n\n\n    print(f'[{epoch + 1:03d}/{num_epoch:03d}] '\n          f'Train Acc: {avg_train_acc:.5f} Loss: {avg_train_loss:.5f} | '\n          f'Val Acc: {avg_val_acc:.5f} Loss: {avg_val_loss:.5f}')\n\n\n    scheduler.step(avg_val_acc)\n\n    if avg_val_acc > best_val_acc:\n        best_val_acc = avg_val_acc\n        torch.save(model.state_dict(), model_path)\n        print(f'Saving model with Val Acc: {best_val_acc:.5f}')\n\ndel train_set, val_set, train_loader, val_loader\ngc.collect()\ntorch.cuda.empty_cache()\nprint(\"--- Training Finished ---\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-23T15:22:33.746245Z","iopub.execute_input":"2025-06-23T15:22:33.746581Z","iopub.status.idle":"2025-06-23T15:38:24.321946Z","shell.execute_reply.started":"2025-06-23T15:22:33.746557Z","shell.execute_reply":"2025-06-23T15:38:24.321029Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"--- Start Testing ---\")\ntest_X = preprocess_data(split='test', feat_dir='kaggle/input/homework2dataset/libriphone/feat', phone_path='kaggle/input/homework2dataset/libriphone', concat_nframes=concat_nframes)\ntest_set = LibriDataset(test_X, None)\ntest_loader = DataLoader(test_set, batch_size=batch_size, shuffle=False, collate_fn=PadSequence())\n\nmodel = LSTMClassifier(\n    input_feature_dim=input_feature_dim_for_lstm,\n    hidden_dim=lstm_hidden_dim,\n    num_layers=lstm_num_layers,\n    output_dim=class_num,\n    dropout_p=lstm_dropout_p,\n    bidirectional=bidirectional\n).to(device)\nmodel.load_state_dict(torch.load(model_path))\n\n\nmodel.eval()\nall_predictions = []\n\nwith torch.no_grad():\n    for batch_idx, features in enumerate(tqdm(test_loader, desc=\"[Test Prediction]\")):\n        features = features.to(device)\n        outputs = model(features)\n\n        _, test_pred = torch.max(outputs, 1)\n        all_predictions.extend(test_pred.cpu().numpy())\n\nwith open('/kaggle/working/prediction.csv', 'w') as f:\n    f.write('Id,Class\\n')\n    for i, y in enumerate(all_predictions):\n        f.write(f'{i},{y}\\n')\n\nprint(\"Prediction file 'prediction.csv' generated.\")\nprint(\"--- Testing Finished ---\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-06-23T16:23:46.489923Z","iopub.execute_input":"2025-06-23T16:23:46.490125Z","iopub.status.idle":"2025-06-23T16:23:46.564753Z","shell.execute_reply.started":"2025-06-23T16:23:46.490108Z","shell.execute_reply":"2025-06-23T16:23:46.563747Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nprint(os.listdir(\"/kaggle/working/.virtual_documents\"))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-23T16:22:35.293753Z","iopub.execute_input":"2025-06-23T16:22:35.294579Z","iopub.status.idle":"2025-06-23T16:22:35.300334Z","shell.execute_reply.started":"2025-06-23T16:22:35.294549Z","shell.execute_reply":"2025-06-23T16:22:35.299180Z"}},"outputs":[],"execution_count":null}]}